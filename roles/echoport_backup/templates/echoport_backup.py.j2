#!/usr/bin/env python3
"""
Echoport self-backup/restore runner for FastDeploy.

Service-owned same-host script for Echoport:
- Backup: SQLite snapshot + .env + optional system files + crontab export,
  manifest, tarball upload to MinIO.
- Restore: Download + checksum verify, safe extract, validation, disk precheck,
  safety snapshot, disable cron, stop service, restore files/db, ownership,
  start service, enable cron, health check, rollback.
"""

import hashlib
import json
import math
import os
import re
import shutil
import subprocess
import sys
import tarfile
import tempfile
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Optional
from urllib.error import URLError
from urllib.request import Request, urlopen

os.environ["PYTHONUNBUFFERED"] = "1"

# Common configuration from Ansible
MC_PATH = "{{ echoport_backup_mc_install_path }}"
MINIO_ALIAS = "{{ echoport_backup_minio_alias }}"
DEFAULT_BUCKET = "{{ echoport_backup_default_bucket }}"
TEMP_DIR = "{{ echoport_backup_temp_dir }}"

# Echoport self-backup configuration from Ansible
SITE_ROOT = "{{ echoport_self_backup_site_root }}"
DB_PATH = "{{ echoport_self_backup_db_path }}"
ENV_FILE = "{{ echoport_self_backup_env_file }}"
LOGS_PATH = "{{ echoport_self_backup_logs_path }}"
SYSTEMD_UNIT_PATH = "{{ echoport_self_backup_systemd_unit_path }}"
TRAEFIK_CONFIG_PATH = "{{ echoport_self_backup_traefik_config_path }}"
CRON_USER = "{{ echoport_self_backup_cron_user }}"
SERVICE_NAME = "{{ echoport_self_backup_service_name }}"
RESTORE_OWNER = "{{ echoport_self_backup_restore_owner }}"

HEALTH_URL = "{{ echoport_self_backup_health_url }}"
HEALTH_EXPECTED_STATUS_RAW = "{{ echoport_self_backup_health_expected_status }}"
HEALTH_RETRIES_RAW = "{{ echoport_self_backup_health_retries }}"
HEALTH_DELAY_RAW = "{{ echoport_self_backup_health_delay }}"
START_TIMEOUT_RAW = "{{ echoport_self_backup_start_timeout }}"

INCLUDE_LOGS_RAW = "{{ echoport_self_backup_include_logs }}"
RESTORE_SYSTEM_FILES_RAW = "{{ echoport_self_backup_restore_system_files }}"
RESTORE_CRONTAB_RAW = "{{ echoport_self_backup_restore_crontab }}"
ENFORCE_HOST_MATCH_RAW = "{{ echoport_self_backup_enforce_host_match }}"
CHECK_DISK_SPACE_RAW = "{{ echoport_self_backup_check_disk_space }}"
DISK_SPACE_MULTIPLIER_RAW = "{{ echoport_self_backup_disk_space_multiplier }}"
CREATE_SAFETY_SNAPSHOT_RAW = "{{ echoport_self_backup_create_safety_snapshot }}"
ROLLBACK_ON_FAILURE_RAW = "{{ echoport_self_backup_rollback_on_failure }}"
CLEANUP_SAFETY_SNAPSHOT_RAW = "{{ echoport_self_backup_cleanup_safety_snapshot }}"

ALLOWED_ROOTS_RAW = "{{ echoport_self_backup_allowed_roots | join(',') }}"

# Echoport managed cron patterns (for selective disable/enable)
MANAGED_CRON_PATTERNS = [
    "run_scheduled_backups",
    "cleanup_old_backups",
]


def parse_bool(value: str, default: bool = False) -> bool:
    if value is None:
        return default
    v = str(value).strip().lower()
    if v in {"1", "true", "yes", "on"}:
        return True
    if v in {"0", "false", "no", "off"}:
        return False
    return default


def parse_int(value: str, default: int) -> int:
    try:
        return int(str(value).strip())
    except Exception:
        return default


def parse_float(value: str, default: float) -> float:
    try:
        return float(str(value).strip())
    except Exception:
        return default


INCLUDE_LOGS = parse_bool(INCLUDE_LOGS_RAW, False)
RESTORE_SYSTEM_FILES = parse_bool(RESTORE_SYSTEM_FILES_RAW, False)
RESTORE_CRONTAB = parse_bool(RESTORE_CRONTAB_RAW, False)
ENFORCE_HOST_MATCH = parse_bool(ENFORCE_HOST_MATCH_RAW, True)
CHECK_DISK_SPACE = parse_bool(CHECK_DISK_SPACE_RAW, True)
DISK_SPACE_MULTIPLIER = parse_float(DISK_SPACE_MULTIPLIER_RAW, 2.0)
CREATE_SAFETY_SNAPSHOT = parse_bool(CREATE_SAFETY_SNAPSHOT_RAW, True)
ROLLBACK_ON_FAILURE = parse_bool(ROLLBACK_ON_FAILURE_RAW, True)
CLEANUP_SAFETY_SNAPSHOT = parse_bool(CLEANUP_SAFETY_SNAPSHOT_RAW, True)

HEALTH_EXPECTED_STATUS = parse_int(HEALTH_EXPECTED_STATUS_RAW, 200)
HEALTH_RETRIES = max(1, parse_int(HEALTH_RETRIES_RAW, 12))
HEALTH_DELAY = max(1, parse_int(HEALTH_DELAY_RAW, 5))
START_TIMEOUT = max(10, parse_int(START_TIMEOUT_RAW, 60))

ALLOWED_ROOTS = [item.strip() for item in ALLOWED_ROOTS_RAW.split(",") if item.strip()]


# ── NDJSON protocol ──────────────────────────────────────────────────────────

def _emit(obj: Dict) -> None:
    print(json.dumps(obj, ensure_ascii=False), flush=True)


def emit_step(name: str, state: str, message: str = "") -> None:
    payload = {"name": name, "state": state}
    if message:
        payload["message"] = message
    _emit(payload)


def emit_result(
    success: bool,
    bucket: str = "",
    key: str = "",
    size_bytes: int = 0,
    checksum_sha256: str = "",
    file_count: int = 0,
    error: str | None = None,
) -> None:
    result = {
        "success": success,
        "bucket": bucket,
        "key": key,
        "size_bytes": size_bytes,
        "checksum_sha256": checksum_sha256,
        "file_count": file_count,
    }
    if error:
        result["error"] = error
    emit_step("result", "success" if success else "failure", f"ECHOPORT_RESULT:{json.dumps(result)}")


def finish_stdout(status: str, message: str = "") -> None:
    payload = {"event": "finish", "status": status}
    if message:
        payload["message"] = message
    _emit(payload)


# ── Helpers ──────────────────────────────────────────────────────────────────

def run_cmd(args: list[str], check: bool = True) -> subprocess.CompletedProcess:
    result = subprocess.run(args, capture_output=True, text=True)
    if check and result.returncode != 0:
        raise subprocess.CalledProcessError(result.returncode, args, result.stdout, result.stderr)
    return result


def calculate_sha256(filepath: Path) -> str:
    sha256_hash = hashlib.sha256()
    with open(filepath, "rb") as handle:
        for chunk in iter(lambda: handle.read(8192), b""):
            sha256_hash.update(chunk)
    return sha256_hash.hexdigest()


def normalize_root(root: str) -> str:
    normalized = os.path.realpath(root)
    if not normalized.endswith(os.sep):
        normalized += os.sep
    return normalized


def path_in_allowed_roots(path: str) -> bool:
    if not path:
        return False
    resolved = os.path.realpath(path)
    for root in ALLOWED_ROOTS:
        normalized_root = normalize_root(root)
        if resolved.startswith(normalized_root) or resolved == normalized_root.rstrip(os.sep):
            return True
    return False


def validate_config_paths() -> None:
    configured = [SITE_ROOT, DB_PATH, ENV_FILE, LOGS_PATH, SYSTEMD_UNIT_PATH, TRAEFIK_CONFIG_PATH]
    for cfg_path in configured:
        if cfg_path and not path_in_allowed_roots(cfg_path):
            raise ValueError(f"Configured path outside allowed roots: {cfg_path}")

    required = [SITE_ROOT, DB_PATH, ENV_FILE]
    for req in required:
        if not Path(req).exists():
            raise ValueError(f"Required path does not exist: {req}")

    if not Path(DB_PATH).is_file():
        raise ValueError(f"Database path is not a file: {DB_PATH}")


def ensure_binaries() -> None:
    needed = [MC_PATH, "sqlite3", "rsync", "tar", "crontab"]
    for binary in needed:
        if binary.startswith("/"):
            if not Path(binary).exists():
                raise FileNotFoundError(f"Required binary not found: {binary}")
        else:
            result = run_cmd(["which", binary], check=False)
            if result.returncode != 0:
                raise FileNotFoundError(f"Required binary not found in PATH: {binary}")


def copy_file_optional(src: Path, dst: Path, mode: str | None = None) -> bool:
    if not src.exists() or not src.is_file():
        return False
    dst.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, dst)
    if mode:
        run_cmd(["chmod", mode, str(dst)])
    return True


def get_temp_parent_dir() -> str | None:
    if TEMP_DIR and Path(TEMP_DIR).exists() and os.access(TEMP_DIR, os.W_OK | os.X_OK):
        return TEMP_DIR
    return None


def get_tree_size(path: Path) -> int:
    if not path.exists():
        return 0
    if path.is_file():
        return path.stat().st_size
    total = 0
    for entry in path.rglob("*"):
        try:
            if entry.is_file():
                total += entry.stat().st_size
        except Exception:
            continue
    return total


def get_free_bytes_for_path(path: Path) -> int:
    candidate = path
    while not candidate.exists() and candidate != candidate.parent:
        candidate = candidate.parent
    stat = os.statvfs(candidate)
    return stat.f_bavail * stat.f_frsize


# ── SQLite operations ────────────────────────────────────────────────────────

def sqlite_backup(source_db: str, backup_path: Path) -> None:
    if not Path(source_db).is_file():
        raise FileNotFoundError(f"SQLite DB not found: {source_db}")
    run_cmd(["sqlite3", source_db, f".backup '{backup_path}'"])
    if not backup_path.exists():
        raise RuntimeError(f"Backup file was not created: {backup_path}")
    integrity = run_cmd(["sqlite3", str(backup_path), "PRAGMA integrity_check;"])
    if "ok" not in integrity.stdout.lower():
        raise RuntimeError(f"SQLite integrity check failed: {integrity.stdout.strip()}")


def sqlite_integrity_check(db_path: str) -> None:
    if not Path(db_path).is_file():
        raise FileNotFoundError(f"SQLite DB not found: {db_path}")
    integrity = run_cmd(["sqlite3", db_path, "PRAGMA integrity_check;"])
    if "ok" not in integrity.stdout.lower():
        raise RuntimeError(f"SQLite integrity check failed: {integrity.stdout.strip()}")


def remove_sqlite_wal_shm(db_path: str) -> None:
    if not db_path:
        return
    for suffix in ("-wal", "-shm"):
        candidate = Path(f"{db_path}{suffix}")
        if candidate.exists():
            candidate.unlink()


# ── Cron operations ──────────────────────────────────────────────────────────

def get_current_crontab() -> str:
    result = run_cmd(["crontab", "-u", CRON_USER, "-l"], check=False)
    if result.returncode != 0:
        return ""
    return result.stdout


def set_crontab(content: str) -> None:
    proc = subprocess.run(
        ["crontab", "-u", CRON_USER, "-"],
        input=content, capture_output=True, text=True,
    )
    if proc.returncode != 0:
        raise RuntimeError(f"Failed to set crontab: {proc.stderr}")


def disable_managed_cron_entries(crontab_content: str) -> str:
    lines = crontab_content.splitlines()
    modified = []
    for line in lines:
        stripped = line.strip()
        if stripped.startswith("#") or not stripped:
            modified.append(line)
            continue
        if any(pattern in line for pattern in MANAGED_CRON_PATTERNS):
            modified.append(f"# ECHOPORT_DISABLED: {line}")
        else:
            modified.append(line)
    return "\n".join(modified) + ("\n" if lines else "")


def enable_managed_cron_entries(crontab_content: str) -> str:
    lines = crontab_content.splitlines()
    modified = []
    for line in lines:
        if line.strip().startswith("# ECHOPORT_DISABLED: "):
            restored = line.strip().replace("# ECHOPORT_DISABLED: ", "", 1)
            modified.append(restored)
        else:
            modified.append(line)
    return "\n".join(modified) + ("\n" if lines else "")


# ── Tarball operations ───────────────────────────────────────────────────────

def make_checksum_manifest(root_dir: Path, manifest_path: Path) -> None:
    entries: list[Path] = []
    for path in root_dir.rglob("*"):
        if not path.is_file():
            continue
        if path.resolve() == manifest_path.resolve():
            continue
        entries.append(path)
    entries.sort(key=lambda p: p.relative_to(root_dir).as_posix())
    lines: list[str] = []
    for item in entries:
        rel = item.relative_to(root_dir).as_posix()
        lines.append(f"{calculate_sha256(item)}  ./{rel}")
    manifest_path.write_text("\n".join(lines) + ("\n" if lines else ""))


def verify_checksum_manifest(root_dir: Path, manifest_path: Path) -> None:
    if not manifest_path.exists():
        raise FileNotFoundError("manifest.sha256 missing from backup archive")
    lines = [line.strip() for line in manifest_path.read_text().splitlines() if line.strip()]
    for line in lines:
        parts = line.split("  ", 1)
        if len(parts) != 2:
            raise ValueError(f"Invalid checksum manifest line: {line}")
        expected, rel_path = parts
        clean_rel = rel_path[2:] if rel_path.startswith("./") else rel_path
        target = root_dir / clean_rel
        if not target.exists() or not target.is_file():
            raise FileNotFoundError(f"Checksum target missing: {clean_rel}")
        actual = calculate_sha256(target)
        if actual != expected:
            raise ValueError(f"Checksum mismatch for {clean_rel}: expected {expected}, got {actual}")


def _is_safe_tar_member(member: tarfile.TarInfo, extract_dir: Path) -> tuple[bool, str]:
    if member.isdev() or member.ischr() or member.isblk():
        return False, f"Tarball contains device node: {member.name}"
    if member.isfifo():
        return False, f"Tarball contains FIFO: {member.name}"
    if member.name.startswith("/"):
        return False, f"Tarball contains absolute path: {member.name}"
    if ".." in member.name:
        return False, f"Tarball contains path traversal: {member.name}"

    target_path = (extract_dir / member.name).resolve()
    try:
        target_path.relative_to(extract_dir.resolve())
    except ValueError:
        return False, f"Tarball member escapes extraction root: {member.name}"

    if member.issym():
        link_target = Path(member.linkname)
        if link_target.is_absolute():
            return False, f"Tarball symlink with absolute target: {member.name} -> {member.linkname}"
        link_resolved = (target_path.parent / link_target).resolve()
        try:
            link_resolved.relative_to(extract_dir.resolve())
        except ValueError:
            return False, f"Tarball symlink escapes extraction root: {member.name} -> {member.linkname}"

    if member.islnk():
        link_target = (extract_dir / member.linkname).resolve()
        try:
            link_target.relative_to(extract_dir.resolve())
        except ValueError:
            return False, f"Tarball hardlink escapes extraction root: {member.name} -> {member.linkname}"

    return True, ""


def safe_extract_tarball(tarball_path: Path, extract_dir: Path) -> None:
    with tarfile.open(tarball_path, "r:gz") as tar:
        safe_members = []
        for member in tar.getmembers():
            is_safe, err = _is_safe_tar_member(member, extract_dir)
            if not is_safe:
                raise ValueError(err)
            safe_members.append(member)
        tar.extractall(extract_dir, members=safe_members)


def create_tarball(source_dir: Path, tarball_path: Path) -> None:
    with tarfile.open(tarball_path, "w:gz") as tar:
        for item in sorted(source_dir.iterdir(), key=lambda p: p.name):
            tar.add(item, arcname=item.name)


# ── Service operations ───────────────────────────────────────────────────────

def stop_service() -> None:
    run_cmd(["systemctl", "stop", SERVICE_NAME])


def start_service() -> None:
    run_cmd(["systemctl", "start", SERVICE_NAME])


def wait_service_active(timeout_seconds: int) -> None:
    deadline = time.time() + timeout_seconds
    while time.time() < deadline:
        result = run_cmd(["systemctl", "is-active", SERVICE_NAME], check=False)
        if result.stdout.strip() == "active":
            return
        time.sleep(2)
    raise RuntimeError(f"Service did not become active in {timeout_seconds}s: {SERVICE_NAME}")


def run_health_check() -> None:
    for attempt in range(1, HEALTH_RETRIES + 1):
        try:
            req = Request(HEALTH_URL, method="GET")
            with urlopen(req, timeout=10) as resp:
                if resp.status == HEALTH_EXPECTED_STATUS:
                    return
        except URLError:
            pass
        except Exception:
            pass
        if attempt < HEALTH_RETRIES:
            time.sleep(HEALTH_DELAY)
    raise RuntimeError(
        f"Health check failed after {HEALTH_RETRIES} attempts: "
        f"{HEALTH_URL} expected {HEALTH_EXPECTED_STATUS}"
    )


def apply_ownership() -> None:
    if not RESTORE_OWNER:
        return
    run_cmd(["chown", "-R", RESTORE_OWNER, SITE_ROOT])
    # Enforce .env mode 0600
    if Path(ENV_FILE).exists():
        run_cmd(["chmod", "0600", ENV_FILE])


# ── Config file reading ─────────────────────────────────────────────────────

def read_config_file() -> Optional[Dict]:
    config_file = os.environ.get("DEPLOY_CONFIG_FILE")
    if "--config" in sys.argv:
        idx = sys.argv.index("--config")
        if idx + 1 < len(sys.argv):
            config_file = sys.argv[idx + 1]
    if not config_file or not Path(config_file).exists():
        return None
    try:
        with open(config_file, "r") as handle:
            return json.load(handle)
    except Exception as exc:
        print(f"Failed to read config file: {exc}", file=sys.stderr)
        return None


# ── Safety snapshot + rollback ───────────────────────────────────────────────

def create_safety_snapshot(snapshot_dir: Path) -> None:
    snapshot_dir.mkdir(parents=True, exist_ok=True)

    # DB
    db_dir = snapshot_dir / "database"
    db_dir.mkdir(parents=True, exist_ok=True)
    if Path(DB_PATH).is_file():
        shutil.copy2(DB_PATH, db_dir / Path(DB_PATH).name)

    # .env
    copy_file_optional(Path(ENV_FILE), snapshot_dir / "config" / "echoport.env")

    # System files
    copy_file_optional(Path(SYSTEMD_UNIT_PATH), snapshot_dir / "system" / "echoport.service")
    copy_file_optional(Path(TRAEFIK_CONFIG_PATH), snapshot_dir / "system" / "echoport.traefik.yml")

    # Crontab
    crontab_content = get_current_crontab()
    if crontab_content:
        cron_dir = snapshot_dir / "system"
        cron_dir.mkdir(parents=True, exist_ok=True)
        (cron_dir / "echoport.crontab").write_text(crontab_content)


def rollback_from_snapshot(snapshot_dir: Path) -> str | None:
    """Attempt rollback from safety snapshot. Returns error message or None."""
    try:
        # Stop service (best effort)
        run_cmd(["systemctl", "stop", SERVICE_NAME], check=False)

        # Restore DB
        db_backup = snapshot_dir / "database" / Path(DB_PATH).name
        if db_backup.exists():
            remove_sqlite_wal_shm(DB_PATH)
            Path(DB_PATH).parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(db_backup, DB_PATH)
            remove_sqlite_wal_shm(DB_PATH)

        # Restore .env
        env_backup = snapshot_dir / "config" / "echoport.env"
        if env_backup.exists():
            Path(ENV_FILE).parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(env_backup, ENV_FILE)

        # Restore system files
        unit_backup = snapshot_dir / "system" / "echoport.service"
        if unit_backup.exists():
            Path(SYSTEMD_UNIT_PATH).parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(unit_backup, SYSTEMD_UNIT_PATH)
            run_cmd(["chmod", "0644", SYSTEMD_UNIT_PATH], check=False)

        traefik_backup = snapshot_dir / "system" / "echoport.traefik.yml"
        if traefik_backup.exists():
            Path(TRAEFIK_CONFIG_PATH).parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(traefik_backup, TRAEFIK_CONFIG_PATH)
            run_cmd(["chmod", "0644", TRAEFIK_CONFIG_PATH], check=False)

        # Ownership + .env mode
        apply_ownership()

        # Restore crontab
        cron_backup = snapshot_dir / "system" / "echoport.crontab"
        if cron_backup.exists():
            set_crontab(cron_backup.read_text())

        # Start service
        start_service()

        return None
    except Exception as exc:
        return str(exc)


# ── Disk precheck ────────────────────────────────────────────────────────────

def disk_space_precheck(tarball_path: Path) -> None:
    if not CHECK_DISK_SPACE:
        return

    with tarfile.open(tarball_path, "r:gz") as tar:
        extracted_payload_bytes = sum(member.size for member in tar.getmembers())

    safety_snapshot_bytes = (
        get_tree_size(Path(DB_PATH))
        + get_tree_size(Path(ENV_FILE))
        + get_tree_size(Path(SYSTEMD_UNIT_PATH))
        + get_tree_size(Path(TRAEFIK_CONFIG_PATH))
        + 4096  # crontab estimate
    )

    required_bytes = int(math.ceil(extracted_payload_bytes * DISK_SPACE_MULTIPLIER) + safety_snapshot_bytes)

    filesystem_paths = [Path(SITE_ROOT), Path(TEMP_DIR)]
    free_bytes = min(get_free_bytes_for_path(p) for p in filesystem_paths)

    if free_bytes < required_bytes:
        raise RuntimeError(
            f"Insufficient disk space. Required={required_bytes} bytes, free={free_bytes} bytes "
            f"(payload={extracted_payload_bytes}, safety_snapshot={safety_snapshot_bytes}, "
            f"multiplier={DISK_SPACE_MULTIPLIER})"
        )


# ── BACKUP FLOW ──────────────────────────────────────────────────────────────

def backup(cenv: Dict) -> int:
    target = cenv.get("ECHOPORT_TARGET", "echoport")
    timestamp = cenv.get("ECHOPORT_TIMESTAMP") or datetime.now(timezone.utc).strftime("%Y-%m-%dT%H-%M-%S")
    key_prefix = cenv.get("ECHOPORT_KEY_PREFIX") or f"{target}/{timestamp}"
    bucket = cenv.get("ECHOPORT_BUCKET", DEFAULT_BUCKET)

    work_dir = Path(tempfile.mkdtemp(prefix="echoport-self-backup-", dir=get_temp_parent_dir()))
    stage_dir = work_dir / "backup"
    stage_dir.mkdir(parents=True, exist_ok=True)

    try:
        emit_step("init", "running", "Validating configuration")
        validate_config_paths()
        ensure_binaries()
        emit_step("init", "success", "Configuration validated")

        # backup_database
        emit_step("backup_database", "running", "Backing up SQLite database")
        db_dir = stage_dir / "database"
        db_dir.mkdir(parents=True, exist_ok=True)
        db_backup_path = db_dir / Path(DB_PATH).name
        sqlite_backup(DB_PATH, db_backup_path)
        db_size = db_backup_path.stat().st_size
        db_checksum = calculate_sha256(db_backup_path)
        emit_step("backup_database", "success", f"Database backup created ({db_size:,} bytes)")

        # copy_config
        emit_step("copy_config", "running", "Copying environment file")
        config_dir = stage_dir / "config"
        config_dir.mkdir(parents=True, exist_ok=True)
        shutil.copy2(ENV_FILE, config_dir / "echoport.env")
        emit_step("copy_config", "success", "Environment file copied")

        # copy_system
        emit_step("copy_system", "running", "Copying system files and crontab")
        system_dir = stage_dir / "system"
        system_dir.mkdir(parents=True, exist_ok=True)

        systemd_present = copy_file_optional(Path(SYSTEMD_UNIT_PATH), system_dir / "echoport.service")
        traefik_present = copy_file_optional(Path(TRAEFIK_CONFIG_PATH), system_dir / "echoport.traefik.yml")

        crontab_content = get_current_crontab()
        crontab_present = False
        if crontab_content:
            (system_dir / "echoport.crontab").write_text(crontab_content)
            crontab_present = True

        emit_step("copy_system", "success", "System files copied")

        # copy_logs
        emit_step("copy_logs", "running", "Copying optional log files")
        logs_included = False
        if INCLUDE_LOGS and Path(LOGS_PATH).exists():
            logs_dir = stage_dir / "logs"
            logs_dir.mkdir(parents=True, exist_ok=True)
            for log_name in ("scheduler.log", "cleanup.log"):
                log_src = Path(LOGS_PATH) / log_name
                if log_src.exists() and log_src.is_file():
                    shutil.copy2(log_src, logs_dir / log_name)
                    logs_included = True
        emit_step("copy_logs", "success", "Log copy complete" if logs_included else "Logs skipped")

        # Build manifest
        manifest = {
            "target": target,
            "timestamp": timestamp,
            "host": os.uname().nodename,
            "run_id": cenv.get("ECHOPORT_RUN_ID", ""),
            "database": {
                "filename": f"database/{Path(DB_PATH).name}",
                "size": db_size,
                "checksum_sha256": db_checksum,
                "type": "sqlite",
            },
            "components": {
                "env": True,
                "systemd": systemd_present,
                "traefik": traefik_present,
                "crontab": crontab_present,
                "logs": logs_included,
            },
            "restore_policies": {
                "enforce_host_match": ENFORCE_HOST_MATCH,
                "restore_system_files": RESTORE_SYSTEM_FILES,
                "restore_crontab": RESTORE_CRONTAB,
            },
        }
        (stage_dir / "manifest.json").write_text(json.dumps(manifest, indent=2))
        make_checksum_manifest(stage_dir, stage_dir / "manifest.sha256")

        # upload
        emit_step("upload", "running", "Creating archive and uploading to MinIO")
        tarball_path = work_dir / f"{timestamp}.tar.gz"
        create_tarball(stage_dir, tarball_path)
        checksum = calculate_sha256(tarball_path)
        size_bytes = tarball_path.stat().st_size
        key = f"{key_prefix}.tar.gz"
        run_cmd([MC_PATH, "cp", str(tarball_path), f"{MINIO_ALIAS}/{bucket}/{key}"])
        emit_step("upload", "success", f"Uploaded {size_bytes:,} bytes")

        # verify
        emit_step("verify", "running", "Verifying uploaded object")
        run_cmd([MC_PATH, "stat", f"{MINIO_ALIAS}/{bucket}/{key}"])
        emit_step("verify", "success", "Backup verified in MinIO")

        file_count = sum(1 for path in stage_dir.rglob("*") if path.is_file())
        emit_result(
            success=True,
            bucket=bucket,
            key=key,
            size_bytes=size_bytes,
            checksum_sha256=checksum,
            file_count=file_count,
        )
        finish_stdout("success", f"Backup completed: {bucket}/{key}")
        return 0

    except Exception as exc:
        print(f"Backup failed: {exc}", file=sys.stderr)
        emit_step("error", "failure", str(exc))
        emit_result(success=False, error=str(exc))
        finish_stdout("failure", f"Backup failed: {exc}")
        return 1

    finally:
        emit_step("cleanup", "running", "Cleaning temporary files")
        shutil.rmtree(work_dir, ignore_errors=True)
        emit_step("cleanup", "success", "Cleanup completed")


# ── RESTORE FLOW ─────────────────────────────────────────────────────────────

def restore(cenv: Dict) -> int:
    target = cenv.get("ECHOPORT_TARGET", "echoport")
    bucket = cenv.get("ECHOPORT_BUCKET", DEFAULT_BUCKET)
    key = cenv.get("ECHOPORT_KEY", "")
    expected_checksum = cenv.get("ECHOPORT_CHECKSUM", "")

    if not key:
        error = "No storage key provided for restore"
        emit_step("init", "failure", error)
        emit_result(success=False, error=error)
        finish_stdout("failure", error)
        return 1

    if not expected_checksum:
        error = "No checksum provided for restore - cannot verify backup integrity"
        emit_step("init", "failure", error)
        emit_result(success=False, error=error)
        finish_stdout("failure", error)
        return 1

    work_dir = Path(tempfile.mkdtemp(prefix="echoport-self-restore-", dir=get_temp_parent_dir()))
    tarball_path = work_dir / "backup.tar.gz"
    extract_dir = work_dir / "extracted"
    extract_dir.mkdir(parents=True, exist_ok=True)

    safety_snapshot_dir = work_dir / "safety"
    destructive_phase_started = False
    rollback_error: str | None = None
    pre_restore_crontab: str | None = None

    try:
        emit_step("init", "running", "Validating restore configuration")
        validate_config_paths()
        ensure_binaries()
        emit_step("init", "success", "Configuration validated")

        # download
        emit_step("download", "running", f"Downloading {bucket}/{key}")
        run_cmd([MC_PATH, "cp", f"{MINIO_ALIAS}/{bucket}/{key}", str(tarball_path)])
        emit_step("download", "success", f"Downloaded backup ({tarball_path.stat().st_size:,} bytes)")

        # verify_checksum
        emit_step("verify_checksum", "running", "Verifying backup checksum")
        actual_checksum = calculate_sha256(tarball_path)
        if actual_checksum != expected_checksum:
            raise ValueError(f"Checksum mismatch: expected {expected_checksum}, got {actual_checksum}")
        emit_step("verify_checksum", "success", "Checksum verified")

        # extract
        emit_step("extract", "running", "Extracting backup archive")
        safe_extract_tarball(tarball_path, extract_dir)
        emit_step("extract", "success", "Archive extracted")

        # validate
        emit_step("validate", "running", "Validating backup contents")
        manifest_path = extract_dir / "manifest.json"
        if not manifest_path.exists():
            raise FileNotFoundError("manifest.json missing from backup archive")

        manifest = json.loads(manifest_path.read_text())

        required_files = [
            extract_dir / manifest.get("database", {}).get("filename", f"database/{Path(DB_PATH).name}"),
            extract_dir / "config" / "echoport.env",
            extract_dir / "manifest.sha256",
        ]
        for path in required_files:
            if not path.exists():
                raise FileNotFoundError(f"Required backup content missing: {path.relative_to(extract_dir)}")

        verify_checksum_manifest(extract_dir, extract_dir / "manifest.sha256")

        if ENFORCE_HOST_MATCH:
            backup_host = manifest.get("host", "")
            current_host = os.uname().nodename
            if backup_host and backup_host != current_host:
                raise ValueError(f"Host mismatch: backup from '{backup_host}', current host '{current_host}'")

        emit_step("validate", "success", "Backup archive validated")

        # disk_precheck
        emit_step("disk_precheck", "running", "Checking available disk space")
        disk_space_precheck(tarball_path)
        emit_step("disk_precheck", "success", "Disk space check passed")

        # safety_snapshot
        emit_step("safety_snapshot", "running", "Creating pre-restore safety snapshot")
        if CREATE_SAFETY_SNAPSHOT:
            create_safety_snapshot(safety_snapshot_dir)
            emit_step("safety_snapshot", "success", "Safety snapshot created")
        else:
            emit_step("safety_snapshot", "success", "Safety snapshot skipped")

        # disable_cron
        emit_step("disable_cron", "running", "Disabling Echoport managed cron entries")
        pre_restore_crontab = get_current_crontab()
        if pre_restore_crontab:
            disabled = disable_managed_cron_entries(pre_restore_crontab)
            set_crontab(disabled)
            emit_step("disable_cron", "success", "Managed cron entries disabled")
        else:
            emit_step("disable_cron", "success", "No crontab to disable")

        # stop_service
        emit_step("stop_service", "running", f"Stopping {SERVICE_NAME}")
        stop_service()
        destructive_phase_started = True
        emit_step("stop_service", "success", f"Stopped {SERVICE_NAME}")

        # restore_files
        emit_step("restore_files", "running", "Restoring .env and optional system files")
        env_src = extract_dir / "config" / "echoport.env"
        Path(ENV_FILE).parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(env_src, ENV_FILE)

        if RESTORE_SYSTEM_FILES:
            system_dir = extract_dir / "system"
            unit_src = system_dir / "echoport.service"
            if unit_src.exists():
                Path(SYSTEMD_UNIT_PATH).parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(unit_src, SYSTEMD_UNIT_PATH)
                run_cmd(["chmod", "0644", SYSTEMD_UNIT_PATH])

            traefik_src = system_dir / "echoport.traefik.yml"
            if traefik_src.exists():
                Path(TRAEFIK_CONFIG_PATH).parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(traefik_src, TRAEFIK_CONFIG_PATH)
                run_cmd(["chmod", "0644", TRAEFIK_CONFIG_PATH])

        crontab_restored_from_archive = False
        if RESTORE_CRONTAB:
            cron_src = extract_dir / "system" / "echoport.crontab"
            if cron_src.exists():
                set_crontab(cron_src.read_text())
                crontab_restored_from_archive = True

        emit_step("restore_files", "success", "Files restored")

        # restore_database
        emit_step("restore_database", "running", "Restoring SQLite database")
        db_rel = manifest.get("database", {}).get("filename", f"database/{Path(DB_PATH).name}")
        db_backup = extract_dir / db_rel

        remove_sqlite_wal_shm(DB_PATH)
        Path(DB_PATH).parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(db_backup, DB_PATH)
        sqlite_integrity_check(DB_PATH)
        remove_sqlite_wal_shm(DB_PATH)
        emit_step("restore_database", "success", "Database restored and verified")

        # set_ownership
        emit_step("set_ownership", "running", f"Applying ownership {RESTORE_OWNER}")
        apply_ownership()
        emit_step("set_ownership", "success", "Ownership applied")

        # start_service
        emit_step("start_service", "running", f"Starting {SERVICE_NAME}")
        start_service()
        wait_service_active(START_TIMEOUT)
        emit_step("start_service", "success", f"Started {SERVICE_NAME}")

        # enable_cron
        emit_step("enable_cron", "running", "Restoring crontab state")
        if crontab_restored_from_archive:
            emit_step("enable_cron", "success", "Crontab already restored from archive")
        elif pre_restore_crontab:
            set_crontab(pre_restore_crontab)
            emit_step("enable_cron", "success", "Pre-restore crontab restored")
        else:
            emit_step("enable_cron", "success", "No crontab to restore")

        # health_check
        emit_step("health_check", "running", "Running health check")
        run_health_check()
        emit_step("health_check", "success", "Health check passed")

        emit_step("rollback", "success", "Rollback not required")

        file_count = sum(1 for path in extract_dir.rglob("*") if path.is_file())
        emit_result(
            success=True,
            bucket=bucket,
            key=key,
            size_bytes=tarball_path.stat().st_size,
            checksum_sha256=actual_checksum,
            file_count=file_count,
        )
        finish_stdout("success", "Restore completed successfully")
        return 0

    except Exception as exc:
        print(f"Restore failed: {exc}", file=sys.stderr)
        message = str(exc)

        emit_step("error", "failure", message)

        rollback_error = None
        if destructive_phase_started and ROLLBACK_ON_FAILURE and CREATE_SAFETY_SNAPSHOT and safety_snapshot_dir.exists():
            emit_step("rollback", "running", "Attempting rollback from safety snapshot")
            rollback_error = rollback_from_snapshot(safety_snapshot_dir)
            if rollback_error:
                emit_step("rollback", "failure", f"Rollback failed: {rollback_error}")
            else:
                emit_step("rollback", "success", "Rollback completed")
        elif destructive_phase_started:
            # Best-effort restart
            emit_step("rollback", "failure", "Rollback skipped: no safety snapshot available")
            run_cmd(["systemctl", "start", SERVICE_NAME], check=False)
            if pre_restore_crontab:
                try:
                    set_crontab(pre_restore_crontab)
                except Exception:
                    pass
        else:
            emit_step("rollback", "success", "Rollback not required")

        if rollback_error:
            message = f"{message}; rollback failed: {rollback_error}"

        emit_result(success=False, error=message)
        finish_stdout("failure", f"Restore failed: {message}")
        return 1

    finally:
        emit_step("cleanup", "running", "Cleaning temporary files")
        if CREATE_SAFETY_SNAPSHOT and CLEANUP_SAFETY_SNAPSHOT and not rollback_error:
            shutil.rmtree(safety_snapshot_dir, ignore_errors=True)
        elif rollback_error:
            emit_step("cleanup", "success",
                      f"Preserving safety snapshot at {safety_snapshot_dir} for manual recovery (rollback failed)")
        shutil.rmtree(work_dir, ignore_errors=True)
        emit_step("cleanup", "success", "Cleanup completed")


# ── MAIN ─────────────────────────────────────────────────────────────────────

def main() -> int:
    try:
        config = read_config_file() or {}
        context = config.get("context", {}) if isinstance(config, dict) else {}
        env_context = context.get("env", {}) if isinstance(context, dict) else {}
        if not isinstance(env_context, dict):
            env_context = {}
        if not env_context and isinstance(context, dict):
            env_context = context

        def get_ctx(key: str, default: str = "") -> str:
            if key in env_context:
                return str(env_context[key])
            return os.environ.get(key, default)

        action = get_ctx("ECHOPORT_ACTION", "backup").strip().lower()
        cenv = {
            "ECHOPORT_ACTION": action,
            "ECHOPORT_TARGET": get_ctx("ECHOPORT_TARGET", "echoport"),
            "ECHOPORT_RUN_ID": get_ctx("ECHOPORT_RUN_ID", ""),
            "ECHOPORT_TIMESTAMP": get_ctx("ECHOPORT_TIMESTAMP", ""),
            "ECHOPORT_BUCKET": get_ctx("ECHOPORT_BUCKET", DEFAULT_BUCKET),
            "ECHOPORT_KEY_PREFIX": get_ctx("ECHOPORT_KEY_PREFIX", ""),
            "ECHOPORT_KEY": get_ctx("ECHOPORT_KEY", ""),
            "ECHOPORT_CHECKSUM": get_ctx("ECHOPORT_CHECKSUM", ""),
        }

        if action == "restore":
            return restore(cenv)
        if action != "backup":
            raise ValueError(f"Unsupported ECHOPORT_ACTION: {action}")
        return backup(cenv)

    except Exception as exc:
        print(f"Fatal error: {exc}", file=sys.stderr)
        emit_step("error", "failure", str(exc))
        emit_result(success=False, error=str(exc))
        finish_stdout("failure", f"Unexpected error: {exc}")
        return 1


if __name__ == "__main__":
    if os.getuid() != 0:
        script_path = os.path.abspath(__file__)
        result = subprocess.run(["sudo", "-n", script_path] + sys.argv[1:], stdout=sys.stdout, stderr=sys.stderr)
        sys.exit(result.returncode)

    Path(TEMP_DIR).mkdir(parents=True, exist_ok=True)

    try:
        sys.exit(main())
    except Exception as exc:
        print(f"Backup/restore failed with error: {exc}", file=sys.stderr)
        emit_step("error", "failure", str(exc))
        emit_result(success=False, error=str(exc))
        finish_stdout("failure", f"Unexpected error: {exc}")
        sys.exit(1)
