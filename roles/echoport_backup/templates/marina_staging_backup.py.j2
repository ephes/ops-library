#!/usr/bin/env python3
"""
Marina staging backup/restore runner for FastDeploy.

This script runs ON MACMINI and uses SSH/SCP/rsync to access the remote
staging server. Staging has no backup tools, no MinIO access - it's just
a dumb target.

Architecture:
- Backup: Stop service via SSH, copy DB + WAL + SHM from staging via SCP,
          run sqlite3 .backup locally, verify integrity, upload to MinIO
- Restore: Download from MinIO, verify checksum, copy to staging via SCP,
           atomic rename, fix ownership, restart service

Deployed by Ansible from ops-control.
"""
import hashlib
import json
import os
import shlex
import shutil
import subprocess
import sys
import tarfile
import tempfile
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Optional

# Ensure unbuffered output
os.environ["PYTHONUNBUFFERED"] = "1"

# Configuration from Ansible
MC_PATH = "{{ echoport_backup_mc_install_path }}"
MINIO_ALIAS = "{{ echoport_backup_minio_alias }}"
DEFAULT_BUCKET = "{{ echoport_backup_default_bucket }}"
TEMP_DIR = "{{ echoport_backup_temp_dir }}"

# SSH configuration
SSH_OPTS = "-o StrictHostKeyChecking=yes -o BatchMode=yes -o ConnectTimeout=30"

# Security: Allowed remote path roots (defense-in-depth)
ALLOWED_ROOTS = [
{% for root in marina_staging_backup_allowed_roots | default(['/home/marina/site/']) %}
    "{{ root }}",
{% endfor %}
]


def read_config_file() -> Optional[Dict]:
    """Read deployment configuration from secure file if available."""
    config_file = os.environ.get("DEPLOY_CONFIG_FILE")

    if "--config" in sys.argv:
        idx = sys.argv.index("--config")
        if idx + 1 < len(sys.argv):
            config_file = sys.argv[idx + 1]

    if not config_file or not Path(config_file).exists():
        return None

    try:
        with open(config_file, "r") as f:
            return json.load(f)
    except Exception as e:
        print(f"Failed to read config file: {e}", file=sys.stderr)
        return None


def _emit(obj: Dict) -> None:
    """Emit JSON object to stdout for FastDeploy to parse."""
    print(json.dumps(obj, ensure_ascii=False), flush=True)


def emit_step(name: str, state: str, message: str = "") -> None:
    """Output step as NDJSON to stdout."""
    step = {"name": name, "state": state}
    if message:
        step["message"] = message
    _emit(step)


def emit_result(
    success: bool,
    bucket: str = "",
    key: str = "",
    size_bytes: int = 0,
    checksum_sha256: str = "",
    file_count: int = 0,
    error: str = None,
) -> None:
    """
    Output ECHOPORT_RESULT as a step message for Echoport to parse.

    The result is embedded in a step's message field as ECHOPORT_RESULT:{json}
    because FastDeploy only parses valid JSON lines from stdout.
    """
    result = {
        "success": success,
        "bucket": bucket,
        "key": key,
        "size_bytes": size_bytes,
        "checksum_sha256": checksum_sha256,
        "file_count": file_count,
    }
    if error:
        result["error"] = error

    result_json = json.dumps(result)
    emit_step("result", "success" if success else "failure", f"ECHOPORT_RESULT:{result_json}")


def finish_stdout(status: str, message: str = None) -> None:
    """Output finish event to stdout."""
    finish = {"event": "finish", "status": status}
    if message:
        finish["message"] = message
    _emit(finish)


import re

# Strict regex for SSH identifiers (hostname, username)
# Allows alphanumeric, dots, hyphens, underscores - no shell metacharacters
_SSH_IDENTIFIER_RE = re.compile(r"^[a-zA-Z0-9._-]+$")


def _validate_ssh_identifier(value: str, name: str) -> None:
    """
    Validate that a value is safe for use in SSH commands.
    Prevents shell injection via remote_host or remote_user.
    """
    if not value:
        raise ValueError(f"{name} cannot be empty")
    if not _SSH_IDENTIFIER_RE.match(value):
        raise ValueError(
            f"{name} contains invalid characters (must match [a-zA-Z0-9._-]+): {value}"
        )


def ssh(
    remote_host: str, remote_user: str, cmd: str, check: bool = True
) -> subprocess.CompletedProcess:
    """Execute command on remote host via SSH."""
    _validate_ssh_identifier(remote_host, "remote_host")
    _validate_ssh_identifier(remote_user, "remote_user")
    full_cmd = f"ssh {SSH_OPTS} {remote_user}@{remote_host} {cmd}"
    print(f"[SSH] {cmd}", file=sys.stderr)
    result = subprocess.run(full_cmd, shell=True, capture_output=True, text=True)
    if check and result.returncode != 0:
        raise subprocess.CalledProcessError(
            result.returncode, full_cmd, result.stdout, result.stderr
        )
    return result


def scp_from_remote(
    remote_host: str, remote_user: str, remote_path: str, local_path: str
) -> None:
    """Copy file from remote host to local path."""
    _validate_ssh_identifier(remote_host, "remote_host")
    _validate_ssh_identifier(remote_user, "remote_user")
    full_cmd = f"scp {SSH_OPTS} {remote_user}@{remote_host}:{shlex.quote(remote_path)} {shlex.quote(local_path)}"
    print(f"[SCP] {remote_path} -> {local_path}", file=sys.stderr)
    result = subprocess.run(full_cmd, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        raise subprocess.CalledProcessError(
            result.returncode, full_cmd, result.stdout, result.stderr
        )


def scp_to_remote(
    remote_host: str, remote_user: str, local_path: str, remote_path: str
) -> None:
    """Copy file from local to remote host."""
    _validate_ssh_identifier(remote_host, "remote_host")
    _validate_ssh_identifier(remote_user, "remote_user")
    full_cmd = f"scp {SSH_OPTS} {shlex.quote(local_path)} {remote_user}@{remote_host}:{shlex.quote(remote_path)}"
    print(f"[SCP] {local_path} -> {remote_path}", file=sys.stderr)
    result = subprocess.run(full_cmd, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        raise subprocess.CalledProcessError(
            result.returncode, full_cmd, result.stdout, result.stderr
        )


def rsync_from_remote(
    remote_host: str, remote_user: str, remote_path: str, local_path: str
) -> None:
    """Rsync directory from remote host to local path (directories only, trailing slashes)."""
    _validate_ssh_identifier(remote_host, "remote_host")
    _validate_ssh_identifier(remote_user, "remote_user")
    # Use -e to pass SSH options, trailing slashes for directory contents
    full_cmd = (
        f'rsync -a --safe-links -e "ssh {SSH_OPTS}" '
        f"{remote_user}@{remote_host}:{shlex.quote(remote_path)}/ {shlex.quote(local_path)}/"
    )
    print(f"[RSYNC] {remote_path}/ -> {local_path}/", file=sys.stderr)
    result = subprocess.run(full_cmd, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        raise subprocess.CalledProcessError(
            result.returncode, full_cmd, result.stdout, result.stderr
        )


def rsync_to_remote(
    remote_host: str, remote_user: str, local_path: str, remote_path: str
) -> None:
    """Rsync directory from local to remote host (directories only, trailing slashes)."""
    _validate_ssh_identifier(remote_host, "remote_host")
    _validate_ssh_identifier(remote_user, "remote_user")
    full_cmd = (
        f'rsync -a -e "ssh {SSH_OPTS}" '
        f"{shlex.quote(local_path)}/ {remote_user}@{remote_host}:{shlex.quote(remote_path)}/"
    )
    print(f"[RSYNC] {local_path}/ -> {remote_path}/", file=sys.stderr)
    result = subprocess.run(full_cmd, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        raise subprocess.CalledProcessError(
            result.returncode, full_cmd, result.stdout, result.stderr
        )


def validate_path_local(path: str) -> None:
    """
    Validate a path string before any remote operation.
    Rejects paths containing '..' segments.
    """
    if ".." in path:
        raise ValueError(f"Path contains '..': {path}")


def _is_path_under_allowed_root(path: str) -> bool:
    """
    Check if a path is under one of the allowed roots.
    Normalizes roots with trailing slashes to prevent prefix footguns
    (e.g., /home/marina/site matching /home/marina/siteevil).
    """
    for root in ALLOWED_ROOTS:
        # Normalize root to have trailing slash for proper prefix matching
        normalized_root = root if root.endswith("/") else root + "/"
        # Path is allowed if it starts with normalized root,
        # or if it equals the root directory itself (without trailing slash)
        if path.startswith(normalized_root) or path == root.rstrip("/"):
            return True
    return False


def validate_remote_path(
    remote_host: str, remote_user: str, path: str, must_exist: bool = True
) -> Optional[str]:
    """
    Validate and canonicalize a remote path.
    Returns canonicalized path, or None if missing and must_exist=False.
    Raises ValueError for security violations.
    """
    # Reject paths with .. segments before any remote operation
    validate_path_local(path)

    # Check if path exists and canonicalize via remote readlink -f
    try:
        result = ssh(remote_host, remote_user, f"readlink -f {shlex.quote(path)}")
        canonical = result.stdout.strip()
    except subprocess.CalledProcessError:
        if must_exist:
            raise ValueError(f"Path does not exist: {path}")
        return None

    # Check if the path actually exists (readlink -f returns a path even for non-existent files)
    try:
        ssh(remote_host, remote_user, f"test -e {shlex.quote(canonical)}")
    except subprocess.CalledProcessError:
        if must_exist:
            raise ValueError(f"Path does not exist: {path}")
        return None

    # Verify canonical path is within allowed roots
    if not _is_path_under_allowed_root(canonical):
        raise ValueError(f"Path resolves outside allowed roots: {path} -> {canonical}")

    return canonical


def validate_remote_directory(
    remote_host: str, remote_user: str, path: str, must_exist: bool = True
) -> Optional[str]:
    """
    Validate and canonicalize a remote directory path.
    Returns canonicalized path, or None if missing and must_exist=False.
    Raises ValueError for security violations or if path is not a directory.
    """
    canonical = validate_remote_path(remote_host, remote_user, path, must_exist)
    if canonical is None:
        return None

    # Verify it's a directory (backup_files must be directories)
    try:
        ssh(remote_host, remote_user, f"test -d {shlex.quote(canonical)}")
    except subprocess.CalledProcessError:
        raise ValueError(f"backup_files must be directories, not files: {path}")

    return canonical


def validate_restore_path(remote_host: str, remote_user: str, path: str) -> str:
    """
    Validate a restore target path with proper canonicalization.
    If path exists, canonicalize it. If not, canonicalize the parent.
    Returns canonical path or raises ValueError for security violations.
    """
    # Reject paths with .. segments
    validate_path_local(path)

    # Check if path exists
    try:
        ssh(remote_host, remote_user, f"test -e {shlex.quote(path)}")
        path_exists = True
    except subprocess.CalledProcessError:
        path_exists = False

    if path_exists:
        # Path exists - canonicalize it directly
        result = ssh(remote_host, remote_user, f"readlink -f {shlex.quote(path)}")
        canonical = result.stdout.strip()
    else:
        # Path doesn't exist - canonicalize parent directory
        parent = str(Path(path).parent)
        try:
            result = ssh(remote_host, remote_user, f"readlink -f {shlex.quote(parent)}")
            canonical_parent = result.stdout.strip()
        except subprocess.CalledProcessError:
            raise ValueError(f"Parent directory does not exist: {parent}")

        # Verify parent exists
        try:
            ssh(remote_host, remote_user, f"test -d {shlex.quote(canonical_parent)}")
        except subprocess.CalledProcessError:
            raise ValueError(f"Parent is not a directory: {parent}")

        # Construct canonical path
        canonical = f"{canonical_parent}/{Path(path).name}"

    # Verify canonical path is within allowed roots
    if not _is_path_under_allowed_root(canonical):
        raise ValueError(f"Restore path resolves outside allowed roots: {path} -> {canonical}")

    return canonical


def validate_backup_files_unique(backup_files: list) -> None:
    """Ensure backup_files have unique basenames (required for archive layout)."""
    basenames = [Path(p).name for p in backup_files]
    if len(basenames) != len(set(basenames)):
        raise ValueError(f"backup_files must have unique basenames, got duplicates: {basenames}")


def validate_db_path(remote_host: str, remote_user: str, db_path: str) -> str:
    """
    Validate and canonicalize the database path.
    Returns canonicalized path or raises ValueError for security violations.
    """
    # Reject paths with .. segments before any remote operation
    validate_path_local(db_path)

    # Canonicalize via remote readlink -f
    try:
        result = ssh(remote_host, remote_user, f"readlink -f {shlex.quote(db_path)}")
        canonical = result.stdout.strip()
    except subprocess.CalledProcessError:
        raise ValueError(f"Cannot resolve db_path: {db_path}")

    # Verify canonical path is within allowed roots
    if not _is_path_under_allowed_root(canonical):
        raise ValueError(f"db_path resolves outside allowed roots: {db_path} -> {canonical}")

    return canonical


def calculate_sha256(filepath: Path) -> str:
    """Calculate SHA256 checksum of a file."""
    sha256_hash = hashlib.sha256()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            sha256_hash.update(chunk)
    return sha256_hash.hexdigest()


def _is_safe_tar_member(member: tarfile.TarInfo, dest_dir: Path) -> tuple[bool, str]:
    """
    Check if a tar member is safe to extract.

    Prevents path traversal attacks by ensuring:
    1. No special file types (device nodes, FIFOs)
    2. No absolute paths
    3. No paths escaping dest_dir via ..
    4. No symlinks/hardlinks pointing outside dest_dir

    Returns (is_safe, error_message).
    """
    # Reject special file types
    if member.isdev() or member.ischr() or member.isblk():
        return False, f"Tarball contains device node: {member.name}"
    if member.isfifo():
        return False, f"Tarball contains FIFO: {member.name}"

    # Check for absolute paths
    if member.name.startswith("/"):
        return False, f"Tarball contains absolute path: {member.name}"

    # Check for path traversal
    if ".." in member.name:
        return False, f"Tarball contains path traversal: {member.name}"

    # Resolve the target path and ensure it's within dest_dir
    target_path = (dest_dir / member.name).resolve()
    try:
        target_path.relative_to(dest_dir.resolve())
    except ValueError:
        return False, f"Tarball member escapes dest_dir: {member.name}"

    # Check symlinks point within dest_dir
    if member.issym():
        link_target = Path(member.linkname)
        if link_target.is_absolute():
            return False, f"Tarball contains symlink with absolute target: {member.name} -> {member.linkname}"
        # Resolve relative to the symlink's directory
        link_resolved = (target_path.parent / link_target).resolve()
        try:
            link_resolved.relative_to(dest_dir.resolve())
        except ValueError:
            return False, f"Tarball contains symlink escaping dest_dir: {member.name} -> {member.linkname}"

    # Check hardlinks point within dest_dir
    if member.islnk():
        link_target = (dest_dir / member.linkname).resolve()
        try:
            link_target.relative_to(dest_dir.resolve())
        except ValueError:
            return False, f"Tarball contains hardlink escaping dest_dir: {member.name} -> {member.linkname}"

    return True, ""


def safe_extract_tarball(tarball_path: Path, extract_dir: Path) -> None:
    """
    Extract tarball with security validation.
    Rejects absolute paths, path traversal, and unsafe symlinks/hardlinks.
    """
    with tarfile.open(tarball_path, "r:gz") as tar:
        safe_members = []
        for member in tar.getmembers():
            is_safe, error_msg = _is_safe_tar_member(member, extract_dir)
            if not is_safe:
                raise ValueError(error_msg)
            safe_members.append(member)

        # Extract only validated members
        tar.extractall(extract_dir, members=safe_members)


def backup(
    remote_host: str,
    remote_user: str,
    db_path: str,
    backup_files: list,
    service_name: str,
    bucket: str,
    key_prefix: str,
    timestamp: str,
) -> int:
    """
    Perform backup from remote staging server.

    Steps:
    1. Validate db_path and backup_files
    2. Validate paths (skip missing with info)
    3. Stop service for consistency
    4. Copy database + WAL + SHM to macmini
    5. Run sqlite3 .backup locally
    6. Run PRAGMA integrity_check
    7. Copy backup_files via rsync
    8. Create manifest and tarball
    9. Calculate checksum
    10. Upload to MinIO
    11. Restart service

    Returns 0 on success, 1 on failure.
    """
    # Validate backup_files basenames are unique
    if backup_files:
        validate_backup_files_unique(backup_files)

    # Create isolated temp directory for this run
    work_dir = Path(tempfile.mkdtemp(prefix="marina-backup-", dir=TEMP_DIR if Path(TEMP_DIR).exists() else None))
    service_stopped = False
    backup_success = False
    result_data = {}

    try:
        emit_step("init", "running", "Starting backup")

        # Validate db_path against allowed roots
        try:
            validate_db_path(remote_host, remote_user, db_path)
        except ValueError as e:
            emit_step("validate_paths", "failure", str(e))
            emit_result(success=False, error=str(e))
            finish_stdout("failure", str(e))
            return 1

        # Validate all backup_files paths upfront (skip missing with info message)
        valid_backup_files = []
        for backup_path in backup_files:
            try:
                canonical = validate_remote_directory(
                    remote_host, remote_user, backup_path, must_exist=False
                )
                if canonical is None:
                    emit_step("validate_paths", "success", f"Skipping missing path: {backup_path}")
                else:
                    valid_backup_files.append(backup_path)
            except ValueError as e:
                emit_step("validate_paths", "failure", str(e))
                emit_result(success=False, error=str(e))
                finish_stdout("failure", str(e))
                return 1

        emit_step("init", "success", "Configuration validated")

        # Stop service for consistency
        emit_step("stop_service", "running", f"Stopping {service_name}")
        try:
            ssh(remote_host, remote_user, f"systemctl stop {shlex.quote(service_name)}")
            service_stopped = True
            emit_step("stop_service", "success", f"Stopped {service_name}")
        except subprocess.CalledProcessError as e:
            emit_step("stop_service", "failure", f"Failed to stop service: {e.stderr}")
            emit_result(success=False, error=f"Failed to stop service: {e.stderr}")
            finish_stdout("failure", "Failed to stop service")
            return 1

        # Copy database + WAL + SHM to macmini
        emit_step("copy_database", "running", "Copying database files")
        for suffix in ["", "-wal", "-shm"]:
            src = f"{db_path}{suffix}"
            dest = work_dir / f"db.sqlite3{suffix}"
            try:
                scp_from_remote(remote_host, remote_user, src, str(dest))
            except subprocess.CalledProcessError:
                if suffix == "":
                    # Main DB is required
                    emit_step("copy_database", "failure", f"Failed to copy database: {db_path}")
                    emit_result(success=False, error=f"Failed to copy database: {db_path}")
                    finish_stdout("failure", "Database copy failed")
                    return 1
                # WAL/SHM may not exist, that's OK
                print(f"[INFO] {src} not found, skipping", file=sys.stderr)

        emit_step("copy_database", "success", "Database files copied")

        # Create clean snapshot locally (keeps staging dumb)
        emit_step("create_snapshot", "running", "Creating consistent snapshot")
        raw_db = work_dir / "db.sqlite3"
        clean_db = work_dir / "db_backup.sqlite3"

        result = subprocess.run(
            ["sqlite3", str(raw_db), f".backup '{clean_db}'"],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            emit_step("create_snapshot", "failure", f"sqlite3 backup failed: {result.stderr}")
            emit_result(success=False, error=f"sqlite3 backup failed: {result.stderr}")
            finish_stdout("failure", "Snapshot creation failed")
            return 1

        emit_step("create_snapshot", "success", "Snapshot created")

        # Integrity check
        emit_step("integrity_check", "running", "Verifying database integrity")
        result = subprocess.run(
            ["sqlite3", str(clean_db), "PRAGMA integrity_check;"],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0 or "ok" not in result.stdout.lower():
            emit_step("integrity_check", "failure", f"Integrity check failed: {result.stdout}")
            emit_result(success=False, error=f"Integrity check failed: {result.stdout}")
            finish_stdout("failure", "Integrity check failed")
            return 1

        emit_step("integrity_check", "success", "Database integrity verified")

        # Copy backup_files via rsync (directories only)
        file_count = 1  # DB
        for backup_path in valid_backup_files:
            emit_step("copy_files", "running", f"Copying {backup_path}")
            # Use basename as archive name (uniqueness validated above)
            dest = work_dir / Path(backup_path).name
            dest.mkdir(parents=True, exist_ok=True)
            try:
                rsync_from_remote(remote_host, remote_user, backup_path, str(dest))
                emit_step("copy_files", "success", f"Copied {backup_path}")
                file_count += 1
            except subprocess.CalledProcessError as e:
                emit_step("copy_files", "failure", f"Failed to copy {backup_path}: {e.stderr}")
                emit_result(success=False, error=f"Failed to copy {backup_path}: {e.stderr}")
                finish_stdout("failure", "File copy failed")
                return 1

        # Create manifest
        manifest = {
            "target": key_prefix.split("/")[0] if "/" in key_prefix else key_prefix,
            "timestamp": timestamp,
            "database": {
                "source": db_path,
                "filename": "db_backup.sqlite3",
                "size": clean_db.stat().st_size,
            },
            "files": valid_backup_files,
        }
        manifest_path = work_dir / "manifest.json"
        manifest_path.write_text(json.dumps(manifest, indent=2))

        # Create tarball
        emit_step("upload", "running", "Creating archive and uploading")
        tarball_path = work_dir / f"{timestamp}.tar.gz"
        with tarfile.open(tarball_path, "w:gz") as tar:
            tar.add(clean_db, arcname="db_backup.sqlite3")
            tar.add(manifest_path, arcname="manifest.json")
            for backup_path in valid_backup_files:
                tar.add(work_dir / Path(backup_path).name, arcname=Path(backup_path).name)

        # Calculate checksum
        checksum = calculate_sha256(tarball_path)
        size_bytes = tarball_path.stat().st_size

        # Upload to MinIO
        key = f"{key_prefix}.tar.gz"
        result = subprocess.run(
            [MC_PATH, "cp", str(tarball_path), f"{MINIO_ALIAS}/{bucket}/{key}"],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            emit_step("upload", "failure", f"MinIO upload failed: {result.stderr}")
            emit_result(success=False, error=f"MinIO upload failed: {result.stderr}")
            finish_stdout("failure", "Upload failed")
            return 1

        emit_step("upload", "success", f"Uploaded {size_bytes:,} bytes")

        # Verify upload
        emit_step("verify", "running", "Verifying upload")
        result = subprocess.run(
            [MC_PATH, "stat", f"{MINIO_ALIAS}/{bucket}/{key}"],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            emit_step("verify", "failure", "Upload verification failed")
            emit_result(success=False, error="Upload verification failed")
            finish_stdout("failure", "Verification failed")
            return 1

        emit_step("verify", "success", "Backup verified in MinIO")

        # Mark backup as successful - result will be emitted after service restart
        backup_success = True
        result_data = {
            "bucket": bucket,
            "key": key,
            "size_bytes": size_bytes,
            "checksum_sha256": checksum,
            "file_count": file_count,
        }

    except Exception as e:
        print(f"Backup failed with error: {e}", file=sys.stderr)
        emit_step("error", "failure", str(e))
        emit_result(success=False, error=str(e))
        finish_stdout("failure", f"Unexpected error: {e}")
        return 1
    finally:
        # Always restart service before emitting final result
        restart_failed = False
        if service_stopped:
            emit_step("start_service", "running", f"Starting {service_name}")
            try:
                ssh(remote_host, remote_user, f"systemctl start {shlex.quote(service_name)}")
                emit_step("start_service", "success", f"Started {service_name}")
            except subprocess.CalledProcessError as e:
                emit_step("start_service", "failure", f"Failed to restart service: {e.stderr}")
                restart_failed = True

        # Clean up temp directory
        shutil.rmtree(work_dir, ignore_errors=True)

        # Emit final result after service restart attempt
        if backup_success and not restart_failed:
            emit_result(success=True, **result_data)
            finish_stdout("success", f"Backup completed: {result_data.get('bucket')}/{result_data.get('key')}")
            return 0
        elif backup_success and restart_failed:
            emit_result(success=False, error="Backup succeeded but service restart failed")
            finish_stdout("failure", "Service restart failed")
            return 1

    return 1  # Should not reach here, but return failure as safety


def restore(
    remote_host: str,
    remote_user: str,
    db_path: str,
    backup_files: list,
    service_name: str,
    restore_owner: str,
    bucket: str,
    key: str,
    expected_checksum: str,
) -> int:
    """
    Perform restore to remote staging server.

    Steps:
    1. Download backup from MinIO
    2. Verify checksum (fail hard on mismatch)
    3. Extract with safety validation
    4. Validate manifest
    5. Validate db_path and all restore paths
    6. Stop service
    7. Remove WAL/SHM on staging
    8. Copy database to temp file (atomic restore)
    9. Atomic rename
    10. Restore backup_files directories
    11. Fix ownership
    12. Start service

    Returns 0 on success, 1 on failure.
    """
    # Create isolated temp directory for this run
    work_dir = Path(tempfile.mkdtemp(prefix="marina-restore-", dir=TEMP_DIR if Path(TEMP_DIR).exists() else None))
    service_stopped = False
    restore_success = False
    result_data = {}

    try:
        emit_step("init", "running", "Starting restore")
        emit_step("init", "success", "Configuration loaded")

        # Download backup from MinIO
        emit_step("download", "running", "Downloading backup from MinIO")
        tarball_path = work_dir / "backup.tar.gz"
        result = subprocess.run(
            [MC_PATH, "cp", f"{MINIO_ALIAS}/{bucket}/{key}", str(tarball_path)],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            emit_step("download", "failure", f"Download failed: {result.stderr}")
            emit_result(success=False, error=f"Download failed: {result.stderr}")
            finish_stdout("failure", "Download failed")
            return 1

        emit_step("download", "success", f"Downloaded backup ({tarball_path.stat().st_size:,} bytes)")

        # Verify checksum (fail hard on mismatch)
        emit_step("verify_checksum", "running", "Verifying checksum")
        actual_checksum = calculate_sha256(tarball_path)
        if actual_checksum != expected_checksum:
            error_msg = f"Checksum mismatch: expected {expected_checksum}, got {actual_checksum}"
            emit_step("verify_checksum", "failure", error_msg)
            emit_result(success=False, error=error_msg)
            finish_stdout("failure", "Checksum verification failed")
            return 1

        emit_step("verify_checksum", "success", "Backup integrity verified")

        # Extract with safety validation
        emit_step("extract", "running", "Extracting backup")
        extract_dir = work_dir / "extracted"
        extract_dir.mkdir()
        try:
            safe_extract_tarball(tarball_path, extract_dir)
        except ValueError as e:
            emit_step("extract", "failure", str(e))
            emit_result(success=False, error=str(e))
            finish_stdout("failure", "Extraction failed")
            return 1

        # Validate manifest
        manifest_path = extract_dir / "manifest.json"
        if not manifest_path.exists():
            emit_step("extract", "failure", "Missing manifest.json in backup")
            emit_result(success=False, error="Missing manifest.json in backup")
            finish_stdout("failure", "Invalid backup archive")
            return 1

        manifest = json.loads(manifest_path.read_text())

        db_filename = manifest.get("database", {}).get("filename", "db_backup.sqlite3")
        db_backup_path = extract_dir / db_filename
        if not db_backup_path.exists():
            emit_step("extract", "failure", f"Missing database file: {db_filename}")
            emit_result(success=False, error=f"Missing database file: {db_filename}")
            finish_stdout("failure", "Invalid backup archive")
            return 1

        emit_step("extract", "success", "Backup extracted")

        # Validate db_path and all restore paths upfront
        emit_step("validate_paths", "running", "Validating restore paths")
        try:
            # Validate db_path against allowed roots
            validate_restore_path(remote_host, remote_user, db_path)
            # Validate backup_files paths
            for restore_path in manifest.get("files", []):
                validate_restore_path(remote_host, remote_user, restore_path)
            emit_step("validate_paths", "success", "Restore paths validated")
        except ValueError as e:
            emit_step("validate_paths", "failure", str(e))
            emit_result(success=False, error=str(e))
            finish_stdout("failure", "Path validation failed")
            return 1

        # Stop service
        emit_step("stop_service", "running", f"Stopping {service_name}")
        try:
            ssh(remote_host, remote_user, f"systemctl stop {shlex.quote(service_name)}")
            service_stopped = True
            emit_step("stop_service", "success", f"Stopped {service_name}")
        except subprocess.CalledProcessError as e:
            emit_step("stop_service", "failure", f"Failed to stop service: {e.stderr}")
            emit_result(success=False, error=f"Failed to stop service: {e.stderr}")
            finish_stdout("failure", "Failed to stop service")
            return 1

        # Remove WAL/SHM on staging (before restore)
        emit_step("clean_wal", "running", "Cleaning WAL/SHM files")
        try:
            ssh(
                remote_host,
                remote_user,
                f"rm -f {shlex.quote(db_path)}-wal {shlex.quote(db_path)}-shm",
            )
            emit_step("clean_wal", "success", "WAL/SHM files cleaned")
        except subprocess.CalledProcessError as e:
            emit_step("clean_wal", "failure", f"Failed to clean WAL/SHM: {e.stderr}")
            emit_result(success=False, error=f"Failed to clean WAL/SHM: {e.stderr}")
            finish_stdout("failure", "WAL cleanup failed")
            return 1

        # Copy database to temp file (atomic restore)
        emit_step("copy_database", "running", "Copying database")
        temp_db_path = f"{db_path}.tmp"
        try:
            scp_to_remote(remote_host, remote_user, str(db_backup_path), temp_db_path)
        except subprocess.CalledProcessError as e:
            emit_step("copy_database", "failure", f"Failed to copy database: {e.stderr}")
            emit_result(success=False, error=f"Failed to copy database: {e.stderr}")
            finish_stdout("failure", "Database copy failed")
            return 1

        # Atomic rename
        emit_step("finalize_database", "running", "Finalizing database")
        try:
            ssh(remote_host, remote_user, f"mv {shlex.quote(temp_db_path)} {shlex.quote(db_path)}")
            emit_step("finalize_database", "success", "Database finalized")
        except subprocess.CalledProcessError as e:
            emit_step("finalize_database", "failure", f"Failed to finalize database: {e.stderr}")
            emit_result(success=False, error=f"Failed to finalize database: {e.stderr}")
            finish_stdout("failure", "Database finalization failed")
            return 1

        # Restore backup_files directories (if any)
        files_restored = 1  # DB
        for restore_path in manifest.get("files", []):
            emit_step("restore_files", "running", f"Restoring {restore_path}")
            src = extract_dir / Path(restore_path).name
            if not src.exists():
                emit_step("restore_files", "failure", f"Backup file missing: {Path(restore_path).name}")
                emit_result(success=False, error=f"Backup file missing: {Path(restore_path).name}")
                finish_stdout("failure", "File restore failed")
                return 1

            try:
                # Ensure target directory exists
                ssh(remote_host, remote_user, f"mkdir -p {shlex.quote(restore_path)}")
                rsync_to_remote(remote_host, remote_user, str(src), restore_path)
                emit_step("restore_files", "success", f"Restored {restore_path}")
                files_restored += 1
            except subprocess.CalledProcessError as e:
                emit_step("restore_files", "failure", f"Failed to restore {restore_path}: {e.stderr}")
                emit_result(success=False, error=f"Failed to restore {restore_path}: {e.stderr}")
                finish_stdout("failure", "File restore failed")
                return 1

        # Fix ownership
        if restore_owner:
            emit_step("set_ownership", "running", f"Setting ownership to {restore_owner}")
            try:
                # Fix DB ownership
                ssh(remote_host, remote_user, f"chown {shlex.quote(restore_owner)} {shlex.quote(db_path)}")
                # Fix backup_files ownership
                for restore_path in manifest.get("files", []):
                    ssh(
                        remote_host,
                        remote_user,
                        f"chown -R {shlex.quote(restore_owner)} {shlex.quote(restore_path)}",
                    )
                emit_step("set_ownership", "success", f"Ownership set to {restore_owner}")
            except subprocess.CalledProcessError as e:
                emit_step("set_ownership", "failure", f"Failed to set ownership: {e.stderr}")
                emit_result(success=False, error=f"Failed to set ownership: {e.stderr}")
                finish_stdout("failure", "Ownership fix failed")
                return 1

        # Mark restore as successful - result will be emitted after service restart
        restore_success = True
        result_data = {
            "bucket": bucket,
            "key": key,
            "size_bytes": tarball_path.stat().st_size,
            "checksum_sha256": actual_checksum,
            "file_count": files_restored,
        }

    except Exception as e:
        print(f"Restore failed with error: {e}", file=sys.stderr)
        emit_step("error", "failure", str(e))
        emit_result(success=False, error=str(e))
        finish_stdout("failure", f"Unexpected error: {e}")
        return 1
    finally:
        # Always restart service before emitting final result
        restart_failed = False
        if service_stopped:
            emit_step("start_service", "running", f"Starting {service_name}")
            try:
                ssh(remote_host, remote_user, f"systemctl start {shlex.quote(service_name)}")
                emit_step("start_service", "success", f"Started {service_name}")
            except subprocess.CalledProcessError as e:
                emit_step("start_service", "failure", f"Failed to restart service: {e.stderr}")
                restart_failed = True

        # Clean up temp directory
        shutil.rmtree(work_dir, ignore_errors=True)

        # Emit final result after service restart attempt
        if restore_success and not restart_failed:
            emit_result(success=True, **result_data)
            finish_stdout("success", f"Restore completed: {result_data.get('file_count')} file(s) restored")
            return 0
        elif restore_success and restart_failed:
            emit_result(success=False, error="Restore succeeded but service restart failed")
            finish_stdout("failure", "Service restart failed")
            return 1

    return 1  # Should not reach here, but return failure as safety


def main() -> int:
    """Main entry point - dispatch to backup or restore."""
    # Read configuration to determine action
    config = read_config_file()

    if config:
        context = config.get("context", {})
    else:
        # Fail fast if running as root without config
        if os.getuid() == 0:
            error_msg = "No config file found while running as root."
            print(f"Error: {error_msg}", file=sys.stderr)
            emit_step("init", "failure", error_msg)
            emit_result(success=False, error=error_msg)
            finish_stdout("failure", "Missing config file")
            return 1

        # Fallback for manual testing
        print("Warning: No config file found, using environment (testing mode)", file=sys.stderr)
        context_raw = os.environ.get("CONTEXT", "{}")
        try:
            context = json.loads(context_raw)
        except json.JSONDecodeError:
            context = {}

    cenv = context.get("env", {})
    action = cenv.get("ECHOPORT_ACTION", "backup")

    # Get common configuration
    # Security: remote_host and remote_user are locked to Ansible defaults
    # to prevent config compromise from redirecting backups to arbitrary hosts
    remote_host = "{{ marina_staging_backup_default_host | default('staging') }}"
    remote_user = "{{ marina_staging_backup_remote_user | default('root') }}"
    db_path = cenv.get("ECHOPORT_DB_PATH", "{{ marina_staging_backup_default_db_path | default('/home/marina/site/db.sqlite3') }}")
    backup_files_str = cenv.get("ECHOPORT_BACKUP_FILES", "")
    backup_files = [f.strip() for f in backup_files_str.split(",") if f.strip()]
    service_name = cenv.get("ECHOPORT_SERVICE_NAME", "{{ marina_staging_backup_service_name | default('marina.service') }}")
    bucket = cenv.get("ECHOPORT_BUCKET", DEFAULT_BUCKET)

    print(f"Configuration:", file=sys.stderr)
    print(f"  Action: {action}", file=sys.stderr)
    print(f"  Remote Host: {remote_host}", file=sys.stderr)
    print(f"  Remote User: {remote_user}", file=sys.stderr)
    print(f"  DB Path: {db_path}", file=sys.stderr)
    print(f"  Backup Files: {backup_files}", file=sys.stderr)
    print(f"  Service: {service_name}", file=sys.stderr)
    print(f"  Bucket: {bucket}", file=sys.stderr)

    if action == "restore":
        key = cenv.get("ECHOPORT_KEY", "")
        expected_checksum = cenv.get("ECHOPORT_CHECKSUM", "")
        restore_owner = cenv.get("ECHOPORT_RESTORE_OWNER", "{{ marina_staging_backup_restore_owner | default('marina:marina') }}")

        if not key:
            error_msg = "No storage key provided for restore"
            emit_step("init", "failure", error_msg)
            emit_result(success=False, error=error_msg)
            finish_stdout("failure", error_msg)
            return 1

        if not expected_checksum:
            error_msg = "No checksum provided for restore - cannot verify backup integrity"
            emit_step("init", "failure", error_msg)
            emit_result(success=False, error=error_msg)
            finish_stdout("failure", error_msg)
            return 1

        print(f"  Key: {key}", file=sys.stderr)
        print(f"  Checksum: {expected_checksum}", file=sys.stderr)
        print(f"  Restore Owner: {restore_owner}", file=sys.stderr)

        return restore(
            remote_host=remote_host,
            remote_user=remote_user,
            db_path=db_path,
            backup_files=backup_files,
            service_name=service_name,
            restore_owner=restore_owner,
            bucket=bucket,
            key=key,
            expected_checksum=expected_checksum,
        )
    else:
        key_prefix = cenv.get("ECHOPORT_KEY_PREFIX", "")
        timestamp = cenv.get("ECHOPORT_TIMESTAMP", datetime.now(timezone.utc).strftime("%Y-%m-%dT%H-%M-%S"))

        if not key_prefix:
            key_prefix = f"marina-staging/{timestamp}"

        print(f"  Key Prefix: {key_prefix}", file=sys.stderr)
        print(f"  Timestamp: {timestamp}", file=sys.stderr)

        return backup(
            remote_host=remote_host,
            remote_user=remote_user,
            db_path=db_path,
            backup_files=backup_files,
            service_name=service_name,
            bucket=bucket,
            key_prefix=key_prefix,
            timestamp=timestamp,
        )


if __name__ == "__main__":
    # Run as root if not already running as root
    # This is needed because FastDeploy runs scripts as 'deploy' user,
    # but we need root access for SSH and file operations
    if os.getuid() != 0:
        # Run via sudo and relay output
        script_path = os.path.abspath(__file__)
        args = ["sudo", "-n", script_path] + sys.argv[1:]
        result = subprocess.run(args, stdout=sys.stdout, stderr=sys.stderr)
        sys.exit(result.returncode)

    # Ensure temp directory exists
    Path(TEMP_DIR).mkdir(parents=True, exist_ok=True)

    try:
        sys.exit(main())
    except Exception as e:
        print(f"Backup/restore failed with error: {e}", file=sys.stderr)
        emit_step("error", "failure", str(e))
        emit_result(success=False, error=str(e))
        finish_stdout("failure", f"Unexpected error: {e}")
        sys.exit(1)
