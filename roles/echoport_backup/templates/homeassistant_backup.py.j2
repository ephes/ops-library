#!/usr/bin/env python3
"""
Home Assistant backup/restore runner for FastDeploy.

Service-owned same-host script for Home Assistant:
- Backup: SQLite snapshots (main + optional zigbee), selective file copies,
  tarball + manifest + checksum manifest, upload to MinIO.
- Restore: download + checksum verify, safe extract, optional host/disk checks,
  safety snapshot, stop services, restore files then DBs, ownership fix,
  restart services in OTBR -> Matter -> HA order, health check.
"""

import hashlib
import json
import os
import shutil
import subprocess
import sys
import tarfile
import tempfile
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Optional
from urllib.error import URLError
from urllib.request import Request, urlopen

# Ensure unbuffered output
os.environ["PYTHONUNBUFFERED"] = "1"

# Common configuration from Ansible
MC_PATH = "{{ echoport_backup_mc_install_path }}"
MINIO_ALIAS = "{{ echoport_backup_minio_alias }}"
DEFAULT_BUCKET = "{{ echoport_backup_default_bucket }}"
TEMP_DIR = "{{ echoport_backup_temp_dir }}"

# Home Assistant-specific configuration from Ansible
SITE_ROOT = "{{ homeassistant_backup_site_root }}"
CONFIG_PATH = "{{ homeassistant_backup_config_path }}"
DATA_PATH = "{{ homeassistant_backup_data_path }}"
LOGS_PATH = "{{ homeassistant_backup_logs_path }}"
MAIN_DB_PATH = "{{ homeassistant_backup_main_db_path }}"
ZIGBEE_DB_PATH = "{{ homeassistant_backup_zigbee_db_path }}"
SERVICE_NAME = "{{ homeassistant_backup_service_name }}"
MATTER_SERVICE_NAME = "{{ homeassistant_backup_matter_service_name }}"
OTBR_SERVICE_NAME = "{{ homeassistant_backup_otbr_service_name }}"
THREAD_STATE_PATH = "{{ homeassistant_backup_thread_state_path }}"
MATTER_CHIP_FACTORY_PATH = "{{ homeassistant_backup_matter_chip_factory_path }}"
SYSTEMD_UNIT_PATH = "{{ homeassistant_backup_systemd_unit_path }}"
TRAEFIK_CONFIG_PATH = "{{ homeassistant_backup_traefik_config_path }}"
RESTORE_OWNER = "{{ homeassistant_backup_restore_owner }}"
HEALTH_CHECK_URL = "{{ homeassistant_backup_health_url }}"
HEALTH_CHECK_FORWARDED_FOR = "{{ homeassistant_backup_health_forwarded_for | default(ansible_default_ipv4.address | default('192.168.0.1')) }}"
HEALTH_CHECK_FORWARDED_PROTO = "{{ homeassistant_backup_health_forwarded_proto | default('http') }}"

INCLUDE_LOGS_RAW = "{{ homeassistant_backup_include_logs | default(true) }}"
RESTORE_SYSTEM_FILES_RAW = "{{ homeassistant_backup_restore_system_files | default(false) }}"
ENFORCE_HOST_MATCH_RAW = "{{ homeassistant_backup_enforce_host_match | default(true) }}"
CHECK_DISK_SPACE_RAW = "{{ homeassistant_backup_check_disk_space | default(true) }}"
DISK_SPACE_MULTIPLIER_RAW = "{{ homeassistant_backup_disk_space_multiplier | default(2.0) }}"
CREATE_SAFETY_SNAPSHOT_RAW = "{{ homeassistant_backup_create_safety_snapshot | default(true) }}"
ROLLBACK_ON_FAILURE_RAW = "{{ homeassistant_backup_rollback_on_failure | default(true) }}"
MANAGE_MATTER_RAW = "{{ homeassistant_backup_manage_matter_server | default(true) }}"
MANAGE_OTBR_RAW = "{{ homeassistant_backup_manage_otbr_state | default(true) }}"
CLEANUP_SAFETY_SNAPSHOT_RAW = "{{ homeassistant_backup_cleanup_safety_snapshot | default(true) }}"

START_TIMEOUT_RAW = "{{ homeassistant_backup_start_timeout | default(120) }}"
START_POLL_INTERVAL_RAW = "{{ homeassistant_backup_start_poll_interval | default(3) }}"
HEALTH_RETRIES_RAW = "{{ homeassistant_backup_health_retries | default(24) }}"
HEALTH_DELAY_RAW = "{{ homeassistant_backup_health_delay | default(5) }}"
HEALTH_EXPECTED_STATUS_RAW = "{{ homeassistant_backup_health_expected_status | default(200) }}"

ALLOWED_ROOTS_RAW = "{{ homeassistant_backup_allowed_roots | join(',') }}"

# Config exclusions for backup (DB files are backed up separately)
CONFIG_EXCLUDES = [
    "home-assistant_v2.db",
    "home-assistant_v2.db-wal",
    "home-assistant_v2.db-shm",
    "zigbee.db",
    "zigbee.db-wal",
    "zigbee.db-shm",
    "deps/",
    "__pycache__/",
    "tts/",
]


def parse_bool(value: str, default: bool = False) -> bool:
    """Parse a bool-like value from template string."""
    if value is None:
        return default
    val = str(value).strip().lower()
    if val in {"1", "true", "yes", "on"}:
        return True
    if val in {"0", "false", "no", "off"}:
        return False
    return default


def parse_float(value: str, default: float) -> float:
    """Parse a float with fallback."""
    try:
        return float(str(value).strip())
    except Exception:
        return default


def parse_int(value: str, default: int) -> int:
    """Parse an int with fallback."""
    try:
        return int(str(value).strip())
    except Exception:
        return default


INCLUDE_LOGS = parse_bool(INCLUDE_LOGS_RAW, True)
RESTORE_SYSTEM_FILES = parse_bool(RESTORE_SYSTEM_FILES_RAW, False)
ENFORCE_HOST_MATCH = parse_bool(ENFORCE_HOST_MATCH_RAW, True)
CHECK_DISK_SPACE = parse_bool(CHECK_DISK_SPACE_RAW, True)
DISK_SPACE_MULTIPLIER = parse_float(DISK_SPACE_MULTIPLIER_RAW, 2.0)
CREATE_SAFETY_SNAPSHOT = parse_bool(CREATE_SAFETY_SNAPSHOT_RAW, True)
ROLLBACK_ON_FAILURE = parse_bool(ROLLBACK_ON_FAILURE_RAW, True)
MANAGE_MATTER = parse_bool(MANAGE_MATTER_RAW, True)
MANAGE_OTBR = parse_bool(MANAGE_OTBR_RAW, True)
CLEANUP_SAFETY_SNAPSHOT = parse_bool(CLEANUP_SAFETY_SNAPSHOT_RAW, True)

START_TIMEOUT = parse_int(START_TIMEOUT_RAW, 120)
START_POLL_INTERVAL = max(1, parse_int(START_POLL_INTERVAL_RAW, 3))
HEALTH_RETRIES = max(1, parse_int(HEALTH_RETRIES_RAW, 24))
HEALTH_DELAY = max(1, parse_int(HEALTH_DELAY_RAW, 5))
HEALTH_EXPECTED_STATUS = parse_int(HEALTH_EXPECTED_STATUS_RAW, 200)

ALLOWED_ROOTS = [item.strip() for item in ALLOWED_ROOTS_RAW.split(",") if item.strip()]


def read_config_file() -> Optional[Dict]:
    """Read deployment configuration from secure file if available."""
    config_file = os.environ.get("DEPLOY_CONFIG_FILE")
    if "--config" in sys.argv:
        idx = sys.argv.index("--config")
        if idx + 1 < len(sys.argv):
            config_file = sys.argv[idx + 1]

    if not config_file or not Path(config_file).exists():
        return None

    try:
        with open(config_file, "r") as handle:
            return json.load(handle)
    except Exception as exc:
        print(f"Failed to read config file: {exc}", file=sys.stderr)
        return None


def _emit(obj: Dict) -> None:
    """Emit JSON object to stdout for FastDeploy to parse."""
    print(json.dumps(obj, ensure_ascii=False), flush=True)


def emit_step(name: str, state: str, message: str = "") -> None:
    """Output a deployment step as NDJSON."""
    payload = {"name": name, "state": state}
    if message:
        payload["message"] = message
    _emit(payload)


def emit_result(
    success: bool,
    bucket: str = "",
    key: str = "",
    size_bytes: int = 0,
    checksum_sha256: str = "",
    file_count: int = 0,
    error: str | None = None,
) -> None:
    """Emit ECHOPORT_RESULT payload in a step message."""
    result = {
        "success": success,
        "bucket": bucket,
        "key": key,
        "size_bytes": size_bytes,
        "checksum_sha256": checksum_sha256,
        "file_count": file_count,
    }
    if error:
        result["error"] = error
    emit_step("result", "success" if success else "failure", f"ECHOPORT_RESULT:{json.dumps(result)}")


def finish_stdout(status: str, message: str = "") -> None:
    """Emit finish event to stdout."""
    payload = {"event": "finish", "status": status}
    if message:
        payload["message"] = message
    _emit(payload)


def run_cmd(args: list[str], check: bool = True) -> subprocess.CompletedProcess:
    """Run command with captured output."""
    result = subprocess.run(args, capture_output=True, text=True)
    if check and result.returncode != 0:
        raise subprocess.CalledProcessError(result.returncode, args, result.stdout, result.stderr)
    return result


def calculate_sha256(filepath: Path) -> str:
    """Calculate SHA256 checksum of a file."""
    sha256_hash = hashlib.sha256()
    with open(filepath, "rb") as handle:
        for chunk in iter(lambda: handle.read(8192), b""):
            sha256_hash.update(chunk)
    return sha256_hash.hexdigest()


def normalize_root(root: str) -> str:
    """Normalize allowlist root for robust prefix checks."""
    normalized = os.path.realpath(root)
    if not normalized.endswith(os.sep):
        normalized += os.sep
    return normalized


def path_in_allowed_roots(path: str) -> bool:
    """Check whether a path is contained in allowed roots."""
    if not path:
        return False
    resolved = os.path.realpath(path)
    for root in ALLOWED_ROOTS:
        normalized_root = normalize_root(root)
        if resolved.startswith(normalized_root) or resolved == normalized_root.rstrip(os.sep):
            return True
    return False


def validate_config_paths() -> None:
    """Validate fixed config paths against allowlist and required existence."""
    required_paths = [SITE_ROOT, CONFIG_PATH, DATA_PATH, MAIN_DB_PATH]
    all_configured_paths = [
        SITE_ROOT,
        CONFIG_PATH,
        DATA_PATH,
        LOGS_PATH,
        MAIN_DB_PATH,
        ZIGBEE_DB_PATH,
        THREAD_STATE_PATH,
        MATTER_CHIP_FACTORY_PATH,
        SYSTEMD_UNIT_PATH,
        TRAEFIK_CONFIG_PATH,
    ]

    for cfg_path in all_configured_paths:
        if cfg_path and not path_in_allowed_roots(cfg_path):
            raise ValueError(f"Configured path outside allowed roots: {cfg_path}")

    for req in required_paths:
        if not Path(req).exists():
            raise ValueError(f"Required path does not exist: {req}")

    if not Path(MAIN_DB_PATH).is_file():
        raise ValueError(f"Main database path is not a file: {MAIN_DB_PATH}")


def sqlite_backup(source_db: str, backup_path: Path) -> None:
    """Create SQLite online backup and verify integrity."""
    if not Path(source_db).is_file():
        raise FileNotFoundError(f"SQLite DB not found: {source_db}")

    run_cmd(["sqlite3", source_db, f".backup '{backup_path}'"])
    if not backup_path.exists():
        raise RuntimeError(f"Backup file was not created: {backup_path}")

    integrity = run_cmd(["sqlite3", str(backup_path), "PRAGMA integrity_check;"])
    if "ok" not in integrity.stdout.lower():
        raise RuntimeError(f"SQLite integrity check failed for backup {backup_path}: {integrity.stdout.strip()}")


def sqlite_integrity_check(db_path: str) -> None:
    """Run PRAGMA integrity_check on an existing SQLite DB."""
    if not Path(db_path).is_file():
        raise FileNotFoundError(f"SQLite DB not found: {db_path}")
    integrity = run_cmd(["sqlite3", db_path, "PRAGMA integrity_check;"])
    if "ok" not in integrity.stdout.lower():
        raise RuntimeError(f"SQLite integrity check failed for {db_path}: {integrity.stdout.strip()}")


def rsync_dir(src_dir: Path, dst_dir: Path, delete: bool = True, excludes: list[str] | None = None) -> None:
    """Rsync directory contents."""
    if not src_dir.exists():
        raise FileNotFoundError(f"Source directory does not exist: {src_dir}")

    dst_dir.mkdir(parents=True, exist_ok=True)

    args = ["rsync", "-a"]
    if delete:
        args.append("--delete")
    for item in excludes or []:
        args.extend(["--exclude", item])
    args.extend([f"{src_dir}/", f"{dst_dir}/"])
    run_cmd(args)


def copy_file_optional(src: Path, dst: Path) -> bool:
    """Copy file if present and return whether copy happened."""
    if not src.exists() or not src.is_file():
        return False
    dst.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, dst)
    return True


def remove_sqlite_wal_shm(db_path: str) -> None:
    """Remove -wal and -shm files if present."""
    if not db_path:
        return
    for suffix in ("-wal", "-shm"):
        candidate = Path(f"{db_path}{suffix}")
        if candidate.exists():
            candidate.unlink()


def service_exists(service_name: str) -> bool:
    """Check whether systemd knows this service."""
    if not service_name:
        return False
    result = run_cmd(["systemctl", "cat", service_name], check=False)
    return result.returncode == 0


def stop_service(service_name: str) -> None:
    """Stop a service."""
    run_cmd(["systemctl", "stop", service_name])


def start_service(service_name: str) -> None:
    """Start a service."""
    run_cmd(["systemctl", "start", service_name])


def wait_service_active(service_name: str, timeout_seconds: int, poll_interval: int) -> None:
    """Wait until a service is active."""
    deadline = time.time() + timeout_seconds
    while time.time() < deadline:
        result = run_cmd(["systemctl", "is-active", service_name], check=False)
        if result.stdout.strip() == "active":
            return
        time.sleep(poll_interval)
    raise RuntimeError(f"Service did not become active in time: {service_name}")


def apply_ownership(path: Path, owner: str) -> None:
    """Apply ownership recursively to path."""
    if not owner:
        return
    run_cmd(["chown", "-R", owner, str(path)])


def _is_safe_tar_member(member: tarfile.TarInfo, extract_dir: Path) -> tuple[bool, str]:
    """Validate tar member for safe extraction."""
    if member.isdev() or member.ischr() or member.isblk():
        return False, f"Tarball contains device node: {member.name}"
    if member.isfifo():
        return False, f"Tarball contains FIFO: {member.name}"
    if member.name.startswith("/"):
        return False, f"Tarball contains absolute path: {member.name}"
    if ".." in member.name:
        return False, f"Tarball contains path traversal: {member.name}"

    target_path = (extract_dir / member.name).resolve()
    try:
        target_path.relative_to(extract_dir.resolve())
    except ValueError:
        return False, f"Tarball member escapes extraction root: {member.name}"

    if member.issym():
        link_target = Path(member.linkname)
        if link_target.is_absolute():
            return False, f"Tarball contains symlink with absolute target: {member.name} -> {member.linkname}"
        link_resolved = (target_path.parent / link_target).resolve()
        try:
            link_resolved.relative_to(extract_dir.resolve())
        except ValueError:
            return False, f"Tarball symlink escapes extraction root: {member.name} -> {member.linkname}"

    if member.islnk():
        link_target = (extract_dir / member.linkname).resolve()
        try:
            link_target.relative_to(extract_dir.resolve())
        except ValueError:
            return False, f"Tarball hardlink escapes extraction root: {member.name} -> {member.linkname}"

    return True, ""


def safe_extract_tarball(tarball_path: Path, extract_dir: Path) -> None:
    """Safely extract tarball contents."""
    with tarfile.open(tarball_path, "r:gz") as tar:
        safe_members = []
        for member in tar.getmembers():
            is_safe, err = _is_safe_tar_member(member, extract_dir)
            if not is_safe:
                raise ValueError(err)
            safe_members.append(member)
        tar.extractall(extract_dir, members=safe_members)


def make_checksum_manifest(root_dir: Path, manifest_path: Path) -> None:
    """Generate SHA256 manifest for all files in root_dir, excluding manifest itself."""
    entries: list[Path] = []
    for path in root_dir.rglob("*"):
        if not path.is_file():
            continue
        if path.resolve() == manifest_path.resolve():
            continue
        entries.append(path)

    entries.sort(key=lambda p: p.relative_to(root_dir).as_posix())
    lines: list[str] = []
    for item in entries:
        rel = item.relative_to(root_dir).as_posix()
        lines.append(f"{calculate_sha256(item)}  ./{rel}")

    manifest_path.write_text("\n".join(lines) + ("\n" if lines else ""))


def verify_checksum_manifest(root_dir: Path, manifest_path: Path) -> None:
    """Verify checksum manifest against extracted files."""
    if not manifest_path.exists():
        raise FileNotFoundError("manifest.sha256 missing from backup archive")

    lines = [line.strip() for line in manifest_path.read_text().splitlines() if line.strip()]
    for line in lines:
        parts = line.split("  ", 1)
        if len(parts) != 2:
            raise ValueError(f"Invalid checksum manifest line: {line}")
        expected, rel_path = parts
        clean_rel = rel_path[2:] if rel_path.startswith("./") else rel_path
        target = root_dir / clean_rel
        if not target.exists() or not target.is_file():
            raise FileNotFoundError(f"Checksum target missing: {clean_rel}")
        actual = calculate_sha256(target)
        if actual != expected:
            raise ValueError(f"Checksum mismatch for {clean_rel}: expected {expected}, got {actual}")


def create_tarball(source_dir: Path, tarball_path: Path) -> None:
    """Create tar.gz archive from source directory contents."""
    with tarfile.open(tarball_path, "w:gz") as tar:
        for item in sorted(source_dir.iterdir(), key=lambda p: p.name):
            tar.add(item, arcname=item.name)


def get_tree_size(path: Path) -> int:
    """Get approximate total size of a directory tree."""
    if not path.exists():
        return 0
    if path.is_file():
        return path.stat().st_size

    total = 0
    for entry in path.rglob("*"):
        try:
            if entry.is_file():
                total += entry.stat().st_size
        except Exception:
            # Best-effort sizing for prechecks.
            continue
    return total


def get_free_bytes_for_path(path: Path) -> int:
    """Get available free bytes on filesystem backing path."""
    candidate = path
    while not candidate.exists() and candidate != candidate.parent:
        candidate = candidate.parent
    stat = os.statvfs(candidate)
    return stat.f_bavail * stat.f_frsize


def disk_space_precheck(tarball_path: Path) -> None:
    """Ensure enough free disk space for extraction and safety snapshot."""
    if not CHECK_DISK_SPACE:
        return

    with tarfile.open(tarball_path, "r:gz") as tar:
        extracted_payload_size = sum(member.size for member in tar.getmembers())

    safety_snapshot_size = get_tree_size(Path(SITE_ROOT)) if (CREATE_SAFETY_SNAPSHOT and Path(SITE_ROOT).exists()) else 0
    required = int(extracted_payload_size * DISK_SPACE_MULTIPLIER + safety_snapshot_size)
    free_bytes = get_free_bytes_for_path(Path(SITE_ROOT))

    if free_bytes < required:
        raise RuntimeError(
            f"Insufficient disk space for restore. Required={required} bytes, free={free_bytes} bytes "
            f"(payload={extracted_payload_size}, safety_snapshot={safety_snapshot_size}, multiplier={DISK_SPACE_MULTIPLIER})"
        )


def read_ha_version() -> str:
    """Read .HA_VERSION from config directory if available."""
    version_file = Path(CONFIG_PATH) / ".HA_VERSION"
    if not version_file.exists():
        return ""
    try:
        return version_file.read_text().strip()
    except Exception:
        return ""


def get_managed_matter_chip_factory_path() -> Path | None:
    """Return chip factory path when it should be managed separately from /data."""
    if not MATTER_CHIP_FACTORY_PATH:
        return None
    normalized = MATTER_CHIP_FACTORY_PATH.rstrip("/") or "/"
    if normalized == "/data":
        return None
    return Path(MATTER_CHIP_FACTORY_PATH)


def count_logical_components(zigbee_present: bool, components: Dict[str, bool]) -> int:
    """Count backed-up logical components for operator-facing visibility."""
    count = 3 + (1 if zigbee_present else 0)  # main DB + config + data (+ optional zigbee DB)
    for key in ("logs", "thread", "matter_chip_factory", "systemd_unit", "traefik_config"):
        if components.get(key, False):
            count += 1
    return count


def backup(cenv: Dict) -> int:
    """Run Home Assistant backup flow."""
    target = cenv.get("ECHOPORT_TARGET", "homeassistant")
    timestamp = cenv.get("ECHOPORT_TIMESTAMP", datetime.now(timezone.utc).strftime("%Y-%m-%dT%H-%M-%S"))
    key_prefix = cenv.get("ECHOPORT_KEY_PREFIX", f"{target}/{timestamp}")
    bucket = cenv.get("ECHOPORT_BUCKET", DEFAULT_BUCKET)

    work_dir = Path(tempfile.mkdtemp(prefix="ha-backup-", dir=TEMP_DIR if Path(TEMP_DIR).exists() else None))
    stage_dir = work_dir / "backup"
    stage_dir.mkdir(parents=True, exist_ok=True)

    try:
        emit_step("init", "running", "Validating configuration")
        validate_config_paths()
        emit_step("init", "success", "Configuration validated")

        db_dir = stage_dir / "database"
        db_dir.mkdir(parents=True, exist_ok=True)

        emit_step("backup_main_db", "running", "Backing up main SQLite database")
        sqlite_backup(MAIN_DB_PATH, db_dir / Path(MAIN_DB_PATH).name)
        emit_step("backup_main_db", "success", "Main database backup created")

        zigbee_source = Path(ZIGBEE_DB_PATH)
        zigbee_backed_up = False
        emit_step("backup_zigbee_db", "running", "Backing up zigbee database (optional)")
        if zigbee_source.exists() and zigbee_source.is_file():
            sqlite_backup(ZIGBEE_DB_PATH, db_dir / zigbee_source.name)
            zigbee_backed_up = True
            emit_step("backup_zigbee_db", "success", "Zigbee database backup created")
        else:
            emit_step("backup_zigbee_db", "success", "Zigbee database not present, skipped")

        emit_step("copy_config", "running", "Copying config directory with exclusions")
        rsync_dir(Path(CONFIG_PATH), stage_dir / "config", delete=True, excludes=CONFIG_EXCLUDES)
        emit_step("copy_config", "success", "Config directory copied")

        emit_step("copy_data", "running", "Copying data directory")
        rsync_dir(Path(DATA_PATH), stage_dir / "data", delete=True)
        emit_step("copy_data", "success", "Data directory copied")

        emit_step("copy_logs", "running", "Copying logs directory")
        logs_included = False
        if INCLUDE_LOGS and Path(LOGS_PATH).exists() and Path(LOGS_PATH).is_dir():
            rsync_dir(Path(LOGS_PATH), stage_dir / "logs", delete=True)
            logs_included = True
        emit_step("copy_logs", "success", "Logs copy complete")

        emit_step("copy_optional", "running", "Copying optional component paths")
        thread_included = False
        if THREAD_STATE_PATH and Path(THREAD_STATE_PATH).exists() and Path(THREAD_STATE_PATH).is_dir():
            rsync_dir(Path(THREAD_STATE_PATH), stage_dir / "thread", delete=True)
            thread_included = True

        managed_chip_factory_path = get_managed_matter_chip_factory_path()
        matter_chip_included = False
        if managed_chip_factory_path and managed_chip_factory_path.exists() and managed_chip_factory_path.is_dir():
            rsync_dir(managed_chip_factory_path, stage_dir / "matter-chip-factory", delete=True)
            matter_chip_included = True

        unit_included = False
        if Path(SYSTEMD_UNIT_PATH).exists() and Path(SYSTEMD_UNIT_PATH).is_file():
            unit_included = copy_file_optional(Path(SYSTEMD_UNIT_PATH), stage_dir / "system" / "homeassistant.service")

        traefik_included = False
        if Path(TRAEFIK_CONFIG_PATH).exists() and Path(TRAEFIK_CONFIG_PATH).is_file():
            traefik_included = copy_file_optional(
                Path(TRAEFIK_CONFIG_PATH),
                stage_dir / "system" / "homeassistant.traefik.yml",
            )

        components = {
            "config": True,
            "data": True,
            "logs": logs_included,
            "thread": thread_included,
            "matter_chip_factory": matter_chip_included,
            "systemd_unit": unit_included,
            "traefik_config": traefik_included,
        }
        file_count = count_logical_components(zigbee_backed_up, components)
        emit_step("copy_optional", "success", "Optional component copy complete")

        manifest = {
            "target": target,
            "timestamp": timestamp,
            "host": os.uname().nodename,
            "database": {
                "main": {
                    "filename": f"database/{Path(MAIN_DB_PATH).name}",
                    "size": (db_dir / Path(MAIN_DB_PATH).name).stat().st_size,
                    "checksum_sha256": calculate_sha256(db_dir / Path(MAIN_DB_PATH).name),
                },
                "zigbee": {
                    "filename": f"database/{zigbee_source.name}",
                    "size": (db_dir / zigbee_source.name).stat().st_size if zigbee_backed_up else 0,
                    "checksum_sha256": calculate_sha256(db_dir / zigbee_source.name) if zigbee_backed_up else "",
                    "present": zigbee_backed_up,
                },
            },
            "components": components,
            "restore_policies": {
                "restore_system_files_default": RESTORE_SYSTEM_FILES,
                "enforce_host_match": ENFORCE_HOST_MATCH,
                "create_safety_snapshot": CREATE_SAFETY_SNAPSHOT,
                "rollback_on_failure": ROLLBACK_ON_FAILURE,
            },
            "ha_version": read_ha_version(),
        }
        (stage_dir / "manifest.json").write_text(json.dumps(manifest, indent=2))
        make_checksum_manifest(stage_dir, stage_dir / "manifest.sha256")

        emit_step("upload", "running", "Creating archive and uploading to MinIO")
        tarball_path = work_dir / f"{timestamp}.tar.gz"
        create_tarball(stage_dir, tarball_path)
        checksum = calculate_sha256(tarball_path)
        size_bytes = tarball_path.stat().st_size
        key = f"{key_prefix}.tar.gz"

        run_cmd([MC_PATH, "cp", str(tarball_path), f"{MINIO_ALIAS}/{bucket}/{key}"])
        emit_step("upload", "success", f"Uploaded {size_bytes:,} bytes")

        emit_step("verify", "running", "Verifying uploaded object")
        run_cmd([MC_PATH, "stat", f"{MINIO_ALIAS}/{bucket}/{key}"])
        emit_step("verify", "success", "Backup verified in MinIO")

        emit_result(
            success=True,
            bucket=bucket,
            key=key,
            size_bytes=size_bytes,
            checksum_sha256=checksum,
            file_count=file_count,
        )
        finish_stdout("success", f"Backup completed: {bucket}/{key}")
        return 0
    except Exception as exc:
        print(f"Backup failed: {exc}", file=sys.stderr)
        emit_step("error", "failure", str(exc))
        emit_result(success=False, error=str(exc))
        finish_stdout("failure", f"Backup failed: {exc}")
        return 1
    finally:
        shutil.rmtree(work_dir, ignore_errors=True)


def maybe_restore_system_files(extract_dir: Path) -> bool:
    """Restore optional /etc files. Returns whether daemon-reload is needed."""
    if not RESTORE_SYSTEM_FILES:
        return False

    daemon_reload_needed = False
    system_dir = extract_dir / "system"
    unit_src = system_dir / "homeassistant.service"
    traefik_src = system_dir / "homeassistant.traefik.yml"

    if unit_src.exists():
        Path(SYSTEMD_UNIT_PATH).parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(unit_src, SYSTEMD_UNIT_PATH)
        run_cmd(["chmod", "0644", SYSTEMD_UNIT_PATH])
        daemon_reload_needed = True

    if traefik_src.exists():
        Path(TRAEFIK_CONFIG_PATH).parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(traefik_src, TRAEFIK_CONFIG_PATH)
        run_cmd(["chmod", "0644", TRAEFIK_CONFIG_PATH])

    return daemon_reload_needed


def run_health_check() -> None:
    """Run HTTP health check with retries."""
    headers = {}
    if HEALTH_CHECK_FORWARDED_FOR:
        headers["X-Forwarded-For"] = HEALTH_CHECK_FORWARDED_FOR
    if HEALTH_CHECK_FORWARDED_PROTO:
        headers["X-Forwarded-Proto"] = HEALTH_CHECK_FORWARDED_PROTO

    for attempt in range(1, HEALTH_RETRIES + 1):
        try:
            req = Request(HEALTH_CHECK_URL, method="GET", headers=headers)
            with urlopen(req, timeout=10) as resp:
                if resp.status == HEALTH_EXPECTED_STATUS:
                    return
        except URLError:
            pass
        except Exception:
            pass

        if attempt < HEALTH_RETRIES:
            time.sleep(HEALTH_DELAY)

    raise RuntimeError(
        f"Health check failed after {HEALTH_RETRIES} attempts: "
        f"{HEALTH_CHECK_URL} expected {HEALTH_EXPECTED_STATUS}"
    )


def restore(cenv: Dict) -> int:
    """Run Home Assistant restore flow."""
    target = cenv.get("ECHOPORT_TARGET", "homeassistant")
    bucket = cenv.get("ECHOPORT_BUCKET", DEFAULT_BUCKET)
    key = cenv.get("ECHOPORT_KEY", "")
    expected_checksum = cenv.get("ECHOPORT_CHECKSUM", "")

    if not key:
        error = "No storage key provided for restore"
        emit_step("init", "failure", error)
        emit_result(success=False, error=error)
        finish_stdout("failure", error)
        return 1

    if not expected_checksum:
        error = "No checksum provided for restore - cannot verify backup integrity"
        emit_step("init", "failure", error)
        emit_result(success=False, error=error)
        finish_stdout("failure", error)
        return 1

    work_dir = Path(tempfile.mkdtemp(prefix="ha-restore-", dir=TEMP_DIR if Path(TEMP_DIR).exists() else None))
    tarball_path = work_dir / "backup.tar.gz"
    extract_dir = work_dir / "extracted"
    extract_dir.mkdir(parents=True, exist_ok=True)

    safety_snapshot_path: Path | None = None
    service_state = {
        "homeassistant_available": service_exists(SERVICE_NAME),
        "matter_available": MANAGE_MATTER and service_exists(MATTER_SERVICE_NAME),
        "otbr_available": MANAGE_OTBR and service_exists(OTBR_SERVICE_NAME),
    }
    services_stopped = {"homeassistant": False, "matter": False, "otbr": False}
    destructive_started = False
    rollback_executed = False
    restore_file_count = 0

    try:
        emit_step("init", "running", "Validating configuration")
        validate_config_paths()
        emit_step("init", "success", "Configuration validated")

        emit_step("download", "running", "Downloading backup from MinIO")
        run_cmd([MC_PATH, "cp", f"{MINIO_ALIAS}/{bucket}/{key}", str(tarball_path)])
        emit_step("download", "success", f"Downloaded backup ({tarball_path.stat().st_size:,} bytes)")

        emit_step("verify_checksum", "running", "Verifying archive checksum")
        actual_checksum = calculate_sha256(tarball_path)
        if actual_checksum != expected_checksum:
            raise RuntimeError(f"Checksum mismatch: expected {expected_checksum}, got {actual_checksum}")
        emit_step("verify_checksum", "success", "Archive checksum verified")

        emit_step("extract", "running", "Extracting backup archive")
        safe_extract_tarball(tarball_path, extract_dir)
        emit_step("extract", "success", "Archive extracted safely")

        emit_step("validate", "running", "Validating manifest and archive contents")
        manifest_path = extract_dir / "manifest.json"
        if not manifest_path.exists():
            raise FileNotFoundError("manifest.json missing from backup archive")

        manifest = json.loads(manifest_path.read_text())
        if ENFORCE_HOST_MATCH:
            backup_host = manifest.get("host", "")
            current_host = os.uname().nodename
            if backup_host and backup_host != current_host:
                raise RuntimeError(
                    f"Backup host mismatch: archive host={backup_host}, current host={current_host}"
                )

        verify_checksum_manifest(extract_dir, extract_dir / "manifest.sha256")

        main_db_rel = manifest.get("database", {}).get("main", {}).get("filename", f"database/{Path(MAIN_DB_PATH).name}")
        main_db_backup = extract_dir / main_db_rel
        if not main_db_backup.exists():
            raise FileNotFoundError(f"Main DB backup missing in archive: {main_db_rel}")

        zigbee_present = bool(manifest.get("database", {}).get("zigbee", {}).get("present", False))
        zigbee_rel = manifest.get("database", {}).get("zigbee", {}).get("filename", f"database/{Path(ZIGBEE_DB_PATH).name}")
        zigbee_backup = extract_dir / zigbee_rel
        if zigbee_present and not zigbee_backup.exists():
            raise FileNotFoundError(f"zigbee DB backup missing in archive: {zigbee_rel}")
        components = manifest.get("components", {})
        restore_file_count = count_logical_components(zigbee_present, components if isinstance(components, dict) else {})

        if CHECK_DISK_SPACE:
            disk_space_precheck(tarball_path)

        emit_step("validate", "success", "Restore validation completed")

        emit_step("safety_snapshot", "running", "Creating pre-restore safety snapshot")
        if CREATE_SAFETY_SNAPSHOT and Path(SITE_ROOT).exists():
            suffix = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
            safety_snapshot_path = Path(f"{SITE_ROOT}.pre-restore-{suffix}")
            safety_snapshot_path.mkdir(parents=True, exist_ok=True)
            rsync_dir(Path(SITE_ROOT), safety_snapshot_path, delete=True)
            emit_step("safety_snapshot", "success", f"Created safety snapshot at {safety_snapshot_path}")
        else:
            emit_step("safety_snapshot", "success", "Safety snapshot disabled or site root missing")

        emit_step("stop_services", "running", "Stopping services before restore")
        if service_state["homeassistant_available"]:
            stop_service(SERVICE_NAME)
            services_stopped["homeassistant"] = True
        if service_state["matter_available"]:
            stop_service(MATTER_SERVICE_NAME)
            services_stopped["matter"] = True
        if service_state["otbr_available"]:
            stop_service(OTBR_SERVICE_NAME)
            services_stopped["otbr"] = True
        destructive_started = True
        emit_step("stop_services", "success", "Services stopped")

        emit_step("restore_files", "running", "Restoring directory contents")
        rsync_dir(extract_dir / "config", Path(CONFIG_PATH), delete=True)
        rsync_dir(extract_dir / "data", Path(DATA_PATH), delete=True)

        logs_backup = extract_dir / "logs"
        if logs_backup.exists():
            rsync_dir(logs_backup, Path(LOGS_PATH), delete=True)

        thread_backup = extract_dir / "thread"
        if thread_backup.exists():
            rsync_dir(thread_backup, Path(THREAD_STATE_PATH), delete=True)

        chip_backup = extract_dir / "matter-chip-factory"
        managed_chip_factory_path = get_managed_matter_chip_factory_path()
        if chip_backup.exists() and managed_chip_factory_path:
            rsync_dir(chip_backup, managed_chip_factory_path, delete=True)

        daemon_reload_needed = maybe_restore_system_files(extract_dir)
        if daemon_reload_needed:
            run_cmd(["systemctl", "daemon-reload"])

        emit_step("restore_files", "success", "Directories restored")

        emit_step("restore_databases", "running", "Restoring SQLite databases")
        remove_sqlite_wal_shm(MAIN_DB_PATH)
        remove_sqlite_wal_shm(ZIGBEE_DB_PATH)

        Path(MAIN_DB_PATH).parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(main_db_backup, MAIN_DB_PATH)
        sqlite_integrity_check(MAIN_DB_PATH)

        if zigbee_present and zigbee_backup.exists():
            Path(ZIGBEE_DB_PATH).parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(zigbee_backup, ZIGBEE_DB_PATH)
            sqlite_integrity_check(ZIGBEE_DB_PATH)

        # Intentional belt-and-suspenders cleanup after DB copy/integrity checks.
        remove_sqlite_wal_shm(MAIN_DB_PATH)
        remove_sqlite_wal_shm(ZIGBEE_DB_PATH)
        emit_step("restore_databases", "success", "Databases restored and verified")

        emit_step("set_ownership", "running", f"Applying ownership {RESTORE_OWNER}")
        apply_ownership(Path(SITE_ROOT), RESTORE_OWNER)
        if (extract_dir / "thread").exists():
            apply_ownership(Path(THREAD_STATE_PATH), "root:root")
        if (extract_dir / "matter-chip-factory").exists() and managed_chip_factory_path:
            apply_ownership(managed_chip_factory_path, RESTORE_OWNER)
        emit_step("set_ownership", "success", "Ownership applied")

        emit_step("start_services", "running", "Starting services")
        if service_state["otbr_available"]:
            start_service(OTBR_SERVICE_NAME)
            services_stopped["otbr"] = False
            wait_service_active(OTBR_SERVICE_NAME, START_TIMEOUT, START_POLL_INTERVAL)
        if service_state["matter_available"]:
            start_service(MATTER_SERVICE_NAME)
            services_stopped["matter"] = False
            wait_service_active(MATTER_SERVICE_NAME, START_TIMEOUT, START_POLL_INTERVAL)
        if service_state["homeassistant_available"]:
            start_service(SERVICE_NAME)
            services_stopped["homeassistant"] = False
            wait_service_active(SERVICE_NAME, START_TIMEOUT, START_POLL_INTERVAL)
        emit_step("start_services", "success", "Services started")

        emit_step("health_check", "running", "Performing Home Assistant health check")
        if service_state["homeassistant_available"]:
            run_health_check()
        emit_step("health_check", "success", "Health check passed")

        if safety_snapshot_path and CLEANUP_SAFETY_SNAPSHOT:
            shutil.rmtree(safety_snapshot_path, ignore_errors=True)

        emit_result(
            success=True,
            bucket=bucket,
            key=key,
            size_bytes=tarball_path.stat().st_size,
            checksum_sha256=actual_checksum,
            file_count=restore_file_count,
        )
        finish_stdout("success", "Restore completed successfully")
        return 0

    except Exception as exc:
        print(f"Restore failed: {exc}", file=sys.stderr)

        if destructive_started and ROLLBACK_ON_FAILURE and safety_snapshot_path and safety_snapshot_path.exists():
            emit_step("rollback", "running", "Attempting rollback from safety snapshot")
            try:
                if service_state["otbr_available"]:
                    run_cmd(["systemctl", "stop", OTBR_SERVICE_NAME], check=False)
                if service_state["matter_available"]:
                    run_cmd(["systemctl", "stop", MATTER_SERVICE_NAME], check=False)
                if service_exists(SERVICE_NAME):
                    run_cmd(["systemctl", "stop", SERVICE_NAME], check=False)
                rsync_dir(safety_snapshot_path, Path(SITE_ROOT), delete=True)
                apply_ownership(Path(SITE_ROOT), RESTORE_OWNER)
                if service_state["otbr_available"]:
                    start_service(OTBR_SERVICE_NAME)
                    wait_service_active(OTBR_SERVICE_NAME, START_TIMEOUT, START_POLL_INTERVAL)
                if service_state["matter_available"]:
                    start_service(MATTER_SERVICE_NAME)
                    wait_service_active(MATTER_SERVICE_NAME, START_TIMEOUT, START_POLL_INTERVAL)
                if service_state["homeassistant_available"]:
                    start_service(SERVICE_NAME)
                    wait_service_active(SERVICE_NAME, START_TIMEOUT, START_POLL_INTERVAL)
                rollback_executed = True
                emit_step("rollback", "success", "Rollback completed")
            except Exception as rollback_exc:
                emit_step("rollback", "failure", f"Rollback failed: {rollback_exc}")
        else:
            emit_step("rollback", "success", "Rollback skipped")

        # Best-effort restart of stopped services if rollback didn't handle it.
        try:
            if services_stopped["otbr"]:
                run_cmd(["systemctl", "start", OTBR_SERVICE_NAME], check=False)
            if services_stopped["matter"]:
                run_cmd(["systemctl", "start", MATTER_SERVICE_NAME], check=False)
            if services_stopped["homeassistant"]:
                run_cmd(["systemctl", "start", SERVICE_NAME], check=False)
        except Exception:
            pass

        suffix = " (rollback executed)" if rollback_executed else ""
        emit_result(success=False, error=f"{exc}{suffix}")
        finish_stdout("failure", f"Restore failed: {exc}{suffix}")
        return 1
    finally:
        emit_step("cleanup", "running", "Cleaning temporary workspace")
        shutil.rmtree(work_dir, ignore_errors=True)
        emit_step("cleanup", "success", "Temporary workspace cleaned")


def main() -> int:
    """Main entry point."""
    config = read_config_file()

    if config:
        context = config.get("context", {})
    else:
        if os.getuid() == 0:
            error = "No config file found while running as root."
            print(error, file=sys.stderr)
            emit_step("init", "failure", error)
            emit_result(success=False, error=error)
            finish_stdout("failure", error)
            return 1

        # Testing fallback (non-production)
        context_raw = os.environ.get("CONTEXT", "{}")
        try:
            context = json.loads(context_raw)
        except json.JSONDecodeError:
            context = {}

    cenv = context.get("env", {})
    action = cenv.get("ECHOPORT_ACTION", "backup")

    if action == "restore":
        return restore(cenv)
    return backup(cenv)


if __name__ == "__main__":
    # Local same-host service needs root for privileged paths.
    if os.getuid() != 0:
        script_path = os.path.abspath(__file__)
        result = subprocess.run(["sudo", "-n", script_path] + sys.argv[1:], stdout=sys.stdout, stderr=sys.stderr)
        sys.exit(result.returncode)

    Path(TEMP_DIR).mkdir(parents=True, exist_ok=True)

    try:
        sys.exit(main())
    except Exception as exc:
        print(f"Backup/restore failed with error: {exc}", file=sys.stderr)
        emit_step("error", "failure", str(exc))
        emit_result(success=False, error=str(exc))
        finish_stdout("failure", f"Unexpected error: {exc}")
        sys.exit(1)
